[{"content":"This blog summaries the lecture 8 of cmu - Intro to Database Systems along with some things i learned on the way.\nConcurrency and Parallelism Parallelism:\nParallelism means running multiple tasks simultaneously. It can be achieved with the help of multiple cores, multiple threads, GPUs, etc.\nExample: In programming, running two different functions at the same time.\nConcurrency:\nConcurrency refers to the ability to run tasks out of order without changing the final outcome. Tasks can progress independently, and generally, these tasks are interleaved. This means the CPU might execute a part of one task, then stop and execute a part of another task, and so on.\nExample: In programming, if a program has two functions, say f1 and f2, where running these two functions in any order does not change the outcome, the result is the same whether f1 runs before f2 or f2 runs before f1. In this case, we can say that the program supports concurrency and these functions are concurrent.\nConcurrency in Index Why do we need the index to be concurrent? We need the index to be concurrent to support the execution of multiple transactions at once and to allow for maximum system throughput. Concurrency enables tasks such as reading, writing, updating, and deleting to be executed out of order. By allowing multiple transactions to be executed out of order, we can run these transactions in parallel while ensuring that the final output remains correct.\nHowever, when multiple transactions modify the index simultaneously, we must handle cases where one transaction might be reading a tuple while another transaction is changing it. In such scenarios, the output expected by the first transaction could be corrupted. To handle these cases, we introduce concurrency control mechanisms.\nWhat do we mean by correct output? There are two main criteria:\nLogical correctness: Logical correctness means that when a thread writes a tuple and then reads the same tuple again, it should see the tuple it wrote initially, provided that no other thread has updated it. This ensures that the data seen by a transaction is consistent with its own operations and the operations of other threads as managed by the concurrency control protocol.\nPhysical correctness: Physical correctness means that the data structure remains sound. This means it is not corrupted, does not hold pointers to invalid memory locations, and does not skip nodes or contain other structural errors. This ensures that the underlying data structure of the index is intact and operational.\nLocks and Latches As we discussed above, if one transaction is reading a node in an index and another transaction comes along and changes the node, the result expected by the first transaction can be corrupted. This means that the first transaction might read part of the node before the other transaction has made changes and the remaining part after the changes have been made. As a result, the output received by the initial transaction will be inconsistent. To handle these issues, we mainly use techniques such as locks, latches, and atomic transactions.\nEven though locks and latches sound similar, they are a bit different:\nLocks: Locks protect the contents of the database, such as tuples and relations, from being modified by other transactions while a transaction is performing operations on them. Locks are held for the entire duration of the transaction and support rollbacks if something goes wrong.\nExample: If a transaction is updating a tuple in a table, a lock is acquired on that tuple to prevent it from being modified by other transactions at the same time.\nLatches: Latches are used to protect the low-level data structures of the DBMS, such as indexes, page tables, etc. Latches are typically held for a short duration and do not need to provide rollbacks.\nExample: Using a latch to protect a node in an index from being modified by other transactions.\nDifferent ways of Latch implementation Blocking OS Mutex\nWe can use the mutex (mutual exclusion) functionality provided by the OS to protect shared data from being modified simultaneously.\nIn mutexes, we acquire a latch on the part of the shared data we are trying to modify. When another thread tries to modify the same part of the shared data, it is not allowed and is put into a waiting queue. Other threads will only be able to access the data when the initial thread holding the latch releases it. Here, the latching and unlatching are handled by the OS, which can be slow.\nA faster alternative in Linux is the futex, which reduces the number of system calls. For DBMSs, the DBMS itself can often do a better job of handling latches for internal data structures.\nTest and Set Spin Latch\nBefore the introduction of the test and set spin latch, the testing and setting of the latch used to be done with two different instructions. Think of a latch as a simple boolean value holding either false (0) or true (1).\nTesting: A thread checks the state of the latch. True means it’s latched, and false means it’s unlatched. Sometimes, it can happen that a thread checks the value of the latch and sees that it’s unlatched. Before it can acquire the latch (set it to true), the thread might be preempted (stopped) for some reason. During this time, another thread might check the latch value, see that it’s unlatched, acquire the latch, and continue its execution. When the CPU resumes the initial thread, it thinks the latch is still unlatched and acquires it. Now, we have two threads doing changes simultaneously, leading to unexpected errors.\nTo resolve this, the test and set instruction is used.\nIn the test and set instruction, testing and setting are combined into one atomic instruction. This ensures that either the thread gets the latch or it doesn’t, with no intermediate state where the latch can be acquired by multiple threads.\nSpin Latch: A spin latch means that the thread will repeatedly execute the test_and_set instruction until it acquires the latch.\nReader-Writer Latches\nThe problem with mutexes and test-and-set latches is that they don’t differentiate between the types of work being done by the threads. For example, if we have 10 threads that want to read some value, using a mutex or test-and-set latch would require each thread to wait for its turn to acquire the lock, which is inefficient.\nReader-writer latches solve this by introducing two types of latches: reader latches and writer latches. When threads are only reading the contents without modifying them, multiple threads can acquire multiple reader latches on the same critical section simultaneously. However, if a thread needs to modify the content, it uses a writer latch. A writer latch is an exclusive latch that can only be acquired when there are no reader or writer latches currently held.\nImplementation of Latches in Hash tables In hash tables, only a single slot is accessed by a thread at a time. By providing latches for the slots, we can support concurrency. This is true, but the implementation can vary based on the size of the critical section being protected.\nThere are two different types of latches based on the size of the critical section they protect:\nPage Latches: Here, latching and unlatching are done at the page level. If a thread wants to access a single slot, it acquires a latch on the entire page. This can put other threads into the waiting queue if they want to access slots in the same page. However, it simplifies the process for the thread holding the latch to read multiple slots within the same page. Slot Latches: Here, latching and unlatching are done at the slot level. If a thread wants to access a single slot, it acquires a latch only on that specific slot. This allows other threads to access different slots within the same page simultaneously. However, this approach requires maintaining a latch for every single slot in the hash table, which can be very expensive. Implementation of Latches in B+Trees implementation of latches in B+Trees is far more complex then the implementation of Latches in the hash table as in B+Trees a single thread might need to access multiple nodes and might also modify multiple nodes.\nUsing Simple Reader-Writer Latches\nWhen using reader-writer latches for indexing searches, we start by acquiring a latch on the root node of the B+Tree. As we proceed, we keep acquiring latches on the child nodes until we reach the leaf node, and finally, we acquire a latch on it. All the latches are held until the completion of the operation.\nIt’s not much of a problem if multiple threads want to search for a key-value pair and only acquire read latches. However, issues arise when we need to make changes. To modify the B+Tree, a thread needs to acquire a write latch. A write latch is exclusive, meaning only one write latch can be held at a time. If a thread wants to insert a new key-value pair into a B+Tree index, it needs to acquire a latch on the root node and maintain it until the leaf node where the insertion will take place. During this time, no other reads or writes can be performed because the root node is latched.\nThis causes a bottleneck and slows down the application. We solve this issue by implementing a few additional rules known as the crabbing latch protocol.\nUsing Crabbing Latch Protocol\nNow, let\u0026rsquo;s see what a crab has to do with latches!\nIn the Crabbing Latch Protocol, we prevent the bottleneck problem by following a pattern of acquiring and releasing latches. For example, if we want to search for a key-value pair, we first acquire a read latch on the root node, then a new read latch on its child node. Once the latch on the child node is acquired, we release the latch on the root node. This pattern of acquiring and releasing latches continues as we traverse down the tree.\nAcquiring write latches is a bit more complex because writing might lead to node splits, merges, or changes in the tree length. Before delving into each case, let\u0026rsquo;s define what a safe node is.\nSafe Node: A safe node is a node that can neither be split nor merged with other nodes if a new value is inserted or deleted from that node.\nSimilar to acquiring read latches, we first acquire a write latch on the root node and then on its child node. We then check if the child node is a safe node. If it is, it guarantees that even if a merge or split happens below the child node, it does not affect the root node(parent node). If the child node is safe, we release the latch on the root node(parent node). If it is not, we keep the latch and continue our traversal down the tree.\nHowever, for each write operation, we need to acquire a write latch on the root node, which can also lead to a bottleneck. Although it\u0026rsquo;s not as severe as before, it still slows down the system. To solve this, we make a slight modification.\nImproved Crabbing Latch Protocol\nThe reading process remains the same as before, but the handling of write latches is slightly modified.\nInitially, we use read latches for write operations as well. We traverse down the tree, acquiring read latches along the way. When we reach the leaf node, we check if we need to split or merge. If there is no need to split or merge, we simply perform the insertion or deletion.\nHowever, if a split or merge is needed, we restart the process, but this time by acquiring the write latches, just as before.\nhandling Leaf Node Scans\nLeaf node scans allow for traversal in different directions: left to right, top to bottom, and right to left. This flexibility can lead to potential deadlocks. For example, if thread A is waiting for thread B to release a latch and vice versa, a deadlock occurs.\nTo handle this situation, the best approach would be for one of the threads to detect the deadlock, wait for a specified amount of time, and then abort itself. This way, the deadlock is resolved, and the aborted thread (e.g., thread A) can restart its operation.\n","permalink":"http://localhost:1313/posts/index-concurrency/","summary":"This blog summaries the lecture 8 of cmu - Intro to Database Systems along with some things i learned on the way.\nConcurrency and Parallelism Parallelism:\nParallelism means running multiple tasks simultaneously. It can be achieved with the help of multiple cores, multiple threads, GPUs, etc.\nExample: In programming, running two different functions at the same time.\nConcurrency:\nConcurrency refers to the ability to run tasks out of order without changing the final outcome.","title":"Index Concurrency"},{"content":"Indexes in DBMS Indexes are very similar to the index section of a book. In books, indexes tell us where a word has been used and provide the page numbers. Just like that, indexes in a Database Management System (DBMS) tell us on what page or block a particular record is present.\nSo why do we need indexes?\nIndexes save us a lot of time. Instead of searching from the start to the end of the book for a specific word, we can simply look up the index and go to the specific pages. Likewise, in DBMS, if we want to retrieve a specific record, instead of searching every record in the database, we can just go to the specific page or block and retrieve the record.\nTypes of Indices Ordered indices: In ordered indices, the index is based on the sorted order of values. Hash indices: In hash indices, the values in the index are distributed over a range of buckets, and the value is hashed to get its index entry and then retrieve the location where the value is stored. Ordered Indices Clustering index: A clustering index is an index where the order in which the values are sorted in the index is the same as the order in which the values are stored in physical memory. Clustering indices reduce random access and are good for sequential access of records. They are also known as primary indices.\nNon-Clustering index: A non-clustering index is an index where the sort order on which the index is based is different from the order in which the values are stored in memory. If we want to access values over a range using non-clustering indices, there will be more random access. They are also known as secondary indices.\nIndex sequential files: files where the records are stored in a sorted order, and they also have an index based on the value on which they are sorted.\nindex entry: The search key value together with a pointer to the actual record in memory is known as an index entry.\nDense Index A dense index is an index where there is an index entry for every record present. In case of duplicate search keys, only a pointer to the first record having that search key is stored, and the rest of the records can be found by a simple sequential access as the index is a clustered index.\nDense indices can also be used on non-clustered indices, but the duplicate keys should be handled differently.\nLookups are performed by just traversing the index table, and if it\u0026rsquo;s based on a clustered index, we can use some optimization such as binary search.\nDense indices\nSparse Index A sparse index is an index where an index entry is present only for a few records. The size of a sparse index is smaller compared to a dense index. Sparse indices always need a clustering index as the lookup of the records which are not in the sparse index is not possible if it\u0026rsquo;s not a clustering index.\nLookups are performed by finding an index entry whose search key value is the greatest search key value present in the sparse index which is less than or equal to the search key value we need, and then doing a sequential search from that record.\nExample: If the search key value is 55, we aim to find an index entry in the sparse index that\u0026rsquo;s less than or equal to 55. If the sparse index contains entries like 45, 23, 25, and 35, we select 45. From there, we navigate to the record containing 45 and start a sequential search.\nSparse indices\nMultilevel indices When an index grows too large to fit in memory, searching for an index entry can become time-consuming due to increased disk I/O operations. To solve this issue, we use multilevel indices.\nIn a multilevel index, the large index table is divided into smaller sub-indices, also known as inner indices. A new index table, often called the outer index or root index, is created, where each index entry points to a specific sub-index of the original large index table. This allows for faster lookup of the relevant sub-index, enabling quicker search for the desired index entry.\nInsertion Dense Index In a dense index, if a new record is added to the database, it is appropriately placed in physical memory, and then a new index entry is made into the dense index, which points to the record.\nIf the dense index stores pointers to all the records having the same search key, under one search key, then a new pointer to the record is just added at the end.\nGiven that the index is a clustering index, If the dense index is storing only a pointer to the first record having the same search key value, then it\u0026rsquo;s not modified. If there is no index entry having the search key of the record being inserted, we create a new index entry.\nSparse Index If we assume an search key entry in a sparse index points to a block of records, and the index is a clustering index, then if a record is being inserted, it is checked if the new record holds the smallest value of the search key in that block. If it is, then we replace the old index entry with the new index entry, which points to the physical location of the newly inserted record. If not, it is not modified.\nDeletion Whenever a record is deleted from a relation, it should be removed from all the indices associated with that relation, so that we can avoid inconsistencies in query results.\nDense Index If a record is being deleted, first the search key value associated with that record is found, and then removed from the index.\nIf the dense index is storing all the pointers to the records having the same search key value, then the pointer to the record being deleted is removed.\nIf the dense index is storing only a pointer to the first record having the same search key value, then it is checked if the record being deleted is the first record. If it is, then the index entry is made to point to the next record having the same search key value.\nSparse Index It is checked if there is an entry of the record in the sparse index. If so, then the index entry is made to point to the next smallest value in that block. If not, the sparse index is not modified.\nSecondary Indices (Non-Clustered Indices) Secondary indices are also known as non-clustered indices. Here, the search key on which the index is constructed upon is different from the search key that the records are sorted and stored in memory.\nHere, the construction of a sparse index is not possible, but the construction of a dense index is possible. Even though the construction of a dense index is possible, the physical location of the values pointed by search keys might be stored in a different order.\nTo increase the access time, the search keys are first found, and then sorted based on their physical location, and then accessed.\nNon-Clustering index\nNon unique search keys If the search keys are non-unique, that is, whenever they are not based on primary keys, then the DBMS will generally concatenate the primary key to create a composite key of the non-unique search key to make it unique and then store it.\nNon-Unique Indices: These are the indices that allow the storage of non-unique search keys.\nB+ Trees The performance of indexes discussed earlier will degrade as the database size increases. However, B+ Tree data structures provide a consistent $O(log(n))$ time complexity for both insertions and deletions. The \u0026ldquo;B\u0026rdquo; in B+ Tree is not formally defined but is generally referred to as \u0026ldquo;balanced.”\nB+ Tree is like a evolution of M-way search tree\nM-way Search Tree An M-way search tree is an evolution of a binary search tree. In a binary search tree, each parent node has two child nodes: the left child and the right child. The value stored in the left child is less than the value of its parent node, and the value stored in the right child is greater than the value of its parent node.\nBinary Search Tree\nIn m-way search trees, the structure of the node is a bit different. Here, each node can store multiple values. An m-way search tree of order n has n pointers to child nodes and has n - 1 values.\nLet\u0026rsquo;s say a node of order n has pointers, p1, p2, ….., pn. and n - 1 values, say v1, v2, v3, … vn-1. The pointer p1 is present at the left end and pointer pn is present at the right end.\nThe values stored in the child node pointed by pointer p1 will be less than the value v1, and the values stored in the child node pointed by the pointer pn will be greater than the value vn-1. Let\u0026rsquo;s say pointer pa is present between values va-1 and va. Here, the child node pointed by the pointer pa stores the values that are less than value va and greater than the value va-1.\nM way node structure\nHere is an example of an M-way search tree of order 4:\nM-way search tree example\nB+ Tree Structure A B+ Tree is a special type of M-way tree with some additional rules:\nevery leaf node is of equal depth from the root node every inner node apart from root node is at least half full, that is number of pointers in a node is always greater then or equal to $⌈(n − 1)∕2⌉$ every inner node with k keys has k+1 non null children In a B+ Tree, the data that is a pointer to a record or the actual contents of the record are only stored in the leaf nodes. Here is the general structure of a B+ Tree:\nB-tree structure\nThe structure of non-leaf nodes is different from leaf nodes. Pointers in non-leaf nodes store a pointer to another non-leaf node or a leaf node.\nIn a leaf node, the value stored may be a pointer to the record containing the search key value or the actual contents of the record.\nHere is the general structure of a leaf node. The last pointer of the node points to the next leaf node.\nLeaf Node\nInsertion Inserting an index is straightforward if there is enough space in the leaf nodes. If we assume there is enough space in the leaf node, we simply find the node where we should add the new entry and add it.\nInsertion becomes more complex when there is not enough space in the leaf nodes, and the leaf node must be split. Here is an example of insertion:\nIf we want to insert an index entry and there is not enough space in the leaf node, the leaf node is split into two nodes. The first ⌈n/2⌉ values stay in the left node, and the rest of the values are moved to the new node. Now, the parent node is updated to include a pointer to the new node.\ninsertion-1\nIf there is not enough space in the parent node to accommodate the new left node created, we split the parent node. We first assume that there is enough space in the parent node by conceptually extending it. Then, we split the node, and the search key value that is present between the pointers that are kept in the left node and the pointers that are moved to the right node is moved up to its parent node.\nIn the example below, it\u0026rsquo;s 46. Here, there is no parent node, so we create a new parent node and add the search key value 46 to it. The new node becomes the root node and also increases the depth of the tree.\ninsertion-2\nDeletion Deletion in B+ trees is a more complex process than insertion. If deleting an index entry does not violate the rule of the node being at least half full, no changes are needed, and we can simply remove the index entry from the leaf node.\nHowever, if the leaf node has fewer pointers than the minimum required, i.e., $n\u0026rsquo; \u0026lt; ⌈(n − 1)∕2⌉$, where n\u0026rsquo; is the number of pointers present in the current node and n is the order of the B+ tree, then the leaf nodes must be merged or pointers in the leaf node must be redistributed.\nIn the example below, we first delete the record 45. Now, the leaf node is left with only one pointer, which points to the search key value containing 40. Since there is enough space in the sibling node, the pointer in the right leaf node can be merged with the left leaf node, and the empty node is deleted, along with the search key in the parent node.\nDeletion-1\nhere is the final state of the B+ tree\nDeletion-2\nNow, let\u0026rsquo;s see an example where we need to merge parent nodes. Assume the below B+ tree, and delete the search key 60.\nDeletion-3\nHere, the search key 60 is first deleted, then the leaf node containing it becomes underfull (it is not half-full). Since the sibling node has enough space to accommodate the remaining pointers, the search key 50 is merged with its sibling, and the search key 60 is removed from its parent node, and the empty node is deleted.\nNow, the parent node is underfull, and we cannot merge the pointers with the sibling node as the sibling node of the parent node is full (30, 40, 43). Here, we redistribute the pointers, moving the rightmost pointer of the sibling node to the right node.\nDeletion-4\nAs the pointers are redistributed, the pointer that separates these two pointers is not present in both nodes, but the search key value is present in the parent node, currently separating them, which is 46.\nHere, the parent node, i.e., the root node, should also be updated to have the correct search key value. The search key value from the sibling node is moved up.\nDeletion-5\nHere is the final state of the B+ tree structure:\nDeletion-6\nSometimes, deletion can reduce the size of the B+ tree. Now, assume the below B-tree:\nDeletion-7\nNow, if we delete the search key entry 47,\ndeletion of 47 from the leaf node makes it underfull. Now, the leaf node cannot be merged with the sibling node, so we redistribute the search keys. The rightmost search key 45 is moved left.\nDeletion-8\nHere, the 46 in the parent node no longer separates the two child nodes, so we correct it by changing it to 45.\nDeletion-9\nNow, we delete 46.\nIf we delete 46, the index entry 45 can now be merged with its sibling node, and the empty right node is deleted.\nDeletion-10\nNow, the parent node is underflowing, and it can now also be merged with its sibling. The search key value separating them is the value present in their parent\u0026rsquo;s node, which is 43.\nDeletion-11\nNow, the root node has only one pointer in it, so the root node is now deleted, and the depth of the entire B+ tree is now reduced.\nHere is the final state of the B+ tree:\nDeletion-12\nDesign Choices Node Size Generally, large node sizes are preferred over smaller ones, as the larger node size reduces the number of hard-disk access times, which can speed up the process. Typically, the size of each node is the same as the size of a single hard-disk page or block.\nMerge Threshold Sometimes, merging of nodes is not carried out in OLTP databases because the number of insertions and deletions is so high that the overhead caused by merging nodes can be significant. The CPU will spend more time merging nodes than actually performing insertions and deletions. This is also known as thrashing.\nVariable Length Keys Sometimes, the length of the search key can vary, for example, if the search key is a name of different lengths.\nPointer Here, a pointer to the search key value is stored instead of storing the search key value directly.\nVariable Length Nodes The search keys are stored normally, but the length of the node can change. This method is not generally used due to the large overhead in managing it.\nPadding The size of the search key is fixed, but for the search keys whose length is less, padding is added to make it fit in. This method is also not generally used due to the memory wastage in padding.\nKey Map/Indirection This is the most commonly used method. The structure of the node is similar to the slotted page, where we have metadata storing some information about the node. We then have an array of pointers pointing to the search key values in the node.\nIntra-Node Search The size of the node in B+trees is generally large, so we also need to use optimised search algorithms to find the required search key in the node.\nHere are some techniques used to search for the required search key:\nLinear Search Every search key in the node is traversed to check if it is the one that is required. The time complexity is $O(n)$.\nBinary Search The binary search technique is used to find the required search key.\n$O(log (n))$\nOptimization Prefix Compression Most search key values stored in a single node are likely to have a part in common. For example: 10001, 10006, 10009. Here, the search keys have 1000 in common. Therefore, only 1000 can be stored, and then 1, 6, 9 can be stored separately, hence saving some space.\nDeduplication Sometimes, the indexes support the insertion of duplicate values. Generally, a primary key is attached at the end to make it a unique composite key. Here, the duplicate search key can only be stored once, and the records can in turn be differentiated by only the primary key, which saves the space lost in storing the duplicate key multiple times.\nBulk Insert Sometimes, if we want to create a new B+tree for a relation, then if we insert the search key of each record one by one, it would lead to a lot of splits and would be slow. Instead, we first get all the search keys and their values, sort them, then turn them into leaf nodes and build the B+ tree from the bottom up.\nhere is the B+tree insertion and deletion example pdf\n","permalink":"http://localhost:1313/posts/tree-indices/","summary":"Indexes in DBMS Indexes are very similar to the index section of a book. In books, indexes tell us where a word has been used and provide the page numbers. Just like that, indexes in a Database Management System (DBMS) tell us on what page or block a particular record is present.\nSo why do we need indexes?\nIndexes save us a lot of time. Instead of searching from the start to the end of the book for a specific word, we can simply look up the index and go to the specific pages.","title":"Tree Indices"},{"content":"This blog is a continuation of the database systems series, covering Lectures 5 and 6 of the Database Systems CMU 15-445/645 course.\nBuffer Pool The buffer pool is the memory present in the main memory (RAM) that is used to store pages that are most frequently used, similar to a cache. It is managed by the DBMS (buffer pool manager), because the DBMS can do a better job of managing pages than the OS. If a query wants to read some records that are present on the disk, the corresponding page should first be fetched into main memory and then read.\nIn the buffer pool, pages are stored in an array of pages where each slot is called a frame.\nBuffer pools are managed by a Buffer Pool Manager.\nBuffer Pool Manager The Buffer Pool Manager is a sub-system of the DBMS that manages the storage present in the main memory (buffer pool). Accessing pages that are present in main memory is much faster than accessing pages that are present on the disk, as they must first be brought into main memory and then read. Therefore, the main goal of a Buffer Pool Manager is to reduce the number of page transfers from disk to main memory, so that most of the pages required by the query are already present in main memory.\nThe Buffer Pool Manager needs to store some additional information to manage the buffer pool.\nPage Table: The page table is an in-memory hash table used for page lookups. It contains a key-value pair where each page ID is mapped to a frame where that page is stored. If a page is needed, the buffer pool manager checks the page table first to see if the page is present, and if it is, it returns the in-memory address of the page.\nIf the page is not present in the page table, it is a page fault. The buffer pool manager first brings the page into main memory, updates the page table, and returns the page address.\nIf the page table is full, one of the pages must be evicted, i.e., one of the pages present in a frame must be replaced with the newly required page.\nNote that this page table is also present in the buffer pool and is managed by the buffer pool manager.er\nDirty Flag: For example, if a query modifies a page, in reality, it is actually modifying the page that is present in main memory, not the page that is present on the disk. So, if any changes are made to the page, the page is marked with a dirty flag. All pages that have been marked with the dirty flag are written back to the disk to persist the data.\nOutput of Pages/Blocks: As we know, pages with dirty flags should be written back to the disk. The output of pages (i.e., writing pages back to the disk) should not be done only when the page needs to be evicted, but should be done frequently or whenever eviction is needed, and writing also depends on other factors. This is to persist the data so that it is safe, and the page can be evicted directly.\nPin/Reference Counter: If a page is being accessed by a thread, the buffer pool manager should make sure that the page is not evicted from the buffer pool. To ensure this, a pin is used. If a thread is accessing a page, the page is pinned, and the buffer pool manager never evicts a pinned page. If the thread completes its operations, it unpins the page, and now the page can be evicted. A single page can be pinned by multiple threads, and a reference counter is used to count the number of threads that are currently accessing the page.\nShare and Exclusive Locks: If multiple threads are accessing a page, say one is reading and another is writing to the page, the write operation performed by a thread changes the contents of the page, which will make the contents read by another thread inconsistent. To handle this, share and exclusive locks are used. If a thread is reading the contents of a page, it obtains a shared lock on that page. A page can have multiple shared locks, as the contents of the page will not be changed, and the threads will have read the same data. If a page has an exclusive lock, the thread must wait until the exclusive lock is removed to obtain a shared lock. If a thread is writing to the page, it obtains an exclusive lock. To obtain an exclusive lock, the page should not have any locks on it, i.e., only one process is allowed to have an exclusive lock at a time.\nMemory Allocation Policies There are two types of memory allocation policies: Global Policies and Local Policies.\nGlobal Policies: These policies make decisions in such a way that the entire workload of a database benefits.\nLocal Policies: These policies make decisions in such a way that only the needs of one transaction are prioritized.\nFor example, if there are four transactions that the database is currently processing, and three of the transactions require the same page, if global policies are being used, the buffer pool manager will bring in the page that is required by most of the transactions. If the DBMS is using local policies, it might just bring in the page that is required by only one transaction, which might be selected among others by policies such as priority, FCFS (First-Come-First-Served), and others.\nBuffer Pool Optimisations Multiple Buffer Pools: A DBMS can have multiple buffer pools, each managed by its own buffer pool manager. Generally, only one type of page is stored in a buffer pool, but a buffer pool can also store pages of more than one type. For example, one buffer pool manager records pages, and another buffer pool manager stores index pages. This increases performance as each buffer pool can have its own policies for page output, page eviction, etc.\nPre-fetching: Pre-fetching means fetching pages from disk into main memory before a request to access the page has been made. The buffer pool manager can determine which pages are most used, depending on the type of workload, previous queries, and a query will be executed in many steps. The buffer pool manager can predict which pages will be requested in the near future by checking the steps being executed and bring the pages into memory before the request to access the pages is made.\nScan Sharing: When accessing records, a pointer traverses each record in a page, processing one at a time. Since a DBMS typically handles multiple queries concurrently, it\u0026rsquo;s possible that two or more queries access the same records. In such cases, the pointer traversing the records can be shared among multiple queries.\nBuffer Pool Bypass: In buffer pool bypass, some part of the buffer pool memory is specifically assigned to a query process, where the records needed by the query are directly loaded and further used. This is done to avoid the overhead of managing pages in the buffer pool, such as tracking pin/unpin pages, dirty pages, etc. This technique is generally used in large sequential scans. If these pages were normally managed by the buffer pool during a large sequential scan, most of the time would be spent evicting pages, and it would also slow down other concurrent processes.\nOS page cache Generally, the OS keeps its own cache of pages loaded into memory. As the DBMS is managing memory on its own, the pages kept in the cache by the OS become redundant. So, the DBMS uses direct I/O to avoid duplicate page caches.\nBuffer Management Policies If a query needs a page that is not currently in the buffer pool, the buffer pool manager should bring the page into memory and return the address of the frame where the page is stored. But if all frames in the buffer pool are full, then any one of the pages present in memory must be replaced to bring the needed page into memory. It is similar to OS CPU scheduling. The buffer pool manager can use policies such as LRU, FIFO, MRU, CLOCK, etc.\nLRU (Least Recently Used): LRU stands for least recently used, as the name suggests. The page that has not been accessed for a long time will be removed from memory and replaced by a new page. To handle LRU, it can be implemented using doubly linked lists (more efficient), where the recently accessed page is moved to the top, and the least recently accessed page is present at the last of the doubly linked list. Alternatively, we can maintain a table (less efficient) containing the timestamps of when the pages are last accessed and update the table whenever new accesses are made. LRU is slow as we need to update the table every time, but it is easy to implement, and it is common that the page that is not being used will most likely not be used in the future.\nFIFO (First In First Out): It is a simple policy where the page that is first brought into memory is selected for eviction when all frames are full. It is similar to the workings of a queue. Similar to LRU, FIFO can be implemented using a linked list (more efficient) or by using a table where the time when the page is loaded into memory is tracked (less efficient).\nCLOCK: Clock is an efficient but approximate implementation of a LRU policy. Here, each frame is associated with a circular data structure, which is used to store the state flag (reference bit), either 0 or 1, and a pointer, also called the clock hand, is used to traverse these flags. If its flag is 1, then 1 is set to 0. If the flag is 0, the page is selected for eviction. Whenever a new page is loaded into main memory, its reference bit/flag is set to 1. The clock hand performs the traversal around the flags whenever all frames are full and a new page is needed to be loaded into main memory.\nA Little Introduction to Indexes: Indexes are used to find the block/page in which the record is stored. For example, if we want to find a record given its ID, we can use the indexes to find the block it is present in and load the block/page into memory and read the record. It is really useful as it saves the work of traversing through every page and checking if the record we want is present there.\nHash Tables A hash table contains an array of buckets. Where each bucket can store one or more records, usually a small fixed size. It uses a hash function h(x)which takes in a key value and produces a new value ranging from 0 to B - 1, where B is the total number of buckets. The value produced by the hash function is used as an index where the key will be stored.\nFor example, consider h(x) = x % 10 , and we want to store a simple record, say “id : 10, name : cosmos”, and we are using id as the key. First, the key is passed into the hash function.\nh(10) = 10 % 10 = 0 , we get 0, we store the record in the bucket whose index is zero.\nThe average time complexity of a hash table is O(1) and the worst time complexity of a hash table is O(n).\nHash functions: An ideal hash function must be faster to compute and must uniformly distribute the keys among all the buckets, and there should be no skew in the distribution of buckets. To avoid skewing of the distribution, one must choose a good hash function, and also in hash tables, usually more buckets are allocated than the total number of keys to further avoid collisions.\nTwo types of hashing: static hashing and dynamic hashing Static hashing: In static hashing, the size of the hashing table is fixed, and the number of keys are known before hashing. The hash function stays constant and is not changed. A disadvantage of static hashing in DBMS is that the number of buckets in the hash table is fixed, and if the memory allocated to the hash table is very much, then it leads to wastage of memory, and if the memory allocated to the hash table is very little, then it leads to more number of collisions and slower lookups.\nDynamic hashing: In dynamic hashing, the size of the hash table can be changed, it can both grow and shrink. The output range of the hash function is also changed along with the size of the table. A disadvantage of a dynamic hash table is mainly about resizing of the hash table, it is an overhead as the memory should be allocated again, and every record in the hash table must be hashed again. The resizing is generally done when the load on DBMS is less.\nCollision handling A collision is said to occur when two or more keys are mapped to the same bucket index. Here, two methods can be used: open addressing and closed addressing\nClosed addressing: Here, a new overflow bucket is used to accommodate the new values in the same bucket index by chaining the buckets (overflow chaining). Closed addressing is generally used in DBMS.\nOpen addressing: Here, the size of the hash table is generally fixed (but can be resized), and no overflow buckets are used. The collisions are handled by placing the new value in some other index of the same hash table, methods such as linear probing, quadratic probing come under this method.\nStatic Hashing techniques Linear Probing Linear probing uses an open addressing approach to handle collisions in the hash table.\nInsertion: In linear probing, if a collision occurs, i.e., there is already a value stored in the bucket (A), then we check if the next bucket (A+1) is empty. If it is, we store the new value there. If not, we again check the next bucket (A+2) and so on. If it is the last bucket in the hash table, we jump to the first bucket in the hash table and continue checking until we find an empty bucket. The initial bucket index is also kept track of so that we don\u0026rsquo;t loop around the buckets.\nDeletion: Deletions are a bit more complex than insertions in linear probing. Say we want to delete a value in the bucket. We hash its key and find its index. Overflow might have occurred, so we must compare the key in the bucket with the key we want. If they are the same, we delete it. If not, we check the next bucket and so on.\nIf there are three values, and if the middle value is deleted, and if we want to get the 3rd value, we do the linear probe again and see that the next value (2nd) is empty. The algorithm might assume that there is no third value, but it\u0026rsquo;s not true. To handle this problem, we use different methods\nTombstone Approach: We store a special value in the bucket, which will indicate that an overflowed value is deleted, and there might be some more overflow values in the next buckets.\nMovement Approach: In this approach, we move all the overflowed buckets up to occupy the newly deleted bucket.\nLinear probing is not generally used in DBMS because the insertion and deletion processes are not efficient, as the number of records might change, and deletions might be performed frequently.\nRobin Hood Hashing Robin Hood is a famous character who steals from the rich and gives to the poor.\nJust like the character Robin Hood, Robin Hood hashing also steals from the rich and gives to the poor. So, how can we say that this value is rich and this is poor? This can be done using probe sequence length.\nProbe Sequence Length: Probe sequence length is the count of the number of buckets we must probe to get to the key we want. For example, say a hash function hashes a key to the index 0, but due to overflow, those buckets are already filled, and say the key is inserted at the index 2. Here, the probe sequence length (PSL) is 2. Note: If PSL is less, the element is rich; if PSL is more, the element is poor.\nInsertion: Insertion in Robin Hood hashing is similar to linear probing. Here, the PSL (probe sequence value) of each key is tracked. If we want to insert a value, we get its key, hash it, and get its index. We check if the bucket at the given index is full. If not, we just insert it there. If it is, we check the next bucket and also check the PSL values (keep in mind that now the PSL value of the key to be inserted is increased by one, as it is now one block away from its hashed index). If the PSL of the key in the bucket is greater than or equal to the PSL of the key to be inserted, we just keep checking further buckets. If the PSL of the key in the bucket is less than the PSL of the key to be inserted (the key in the bucket is considered rich, and the key to be inserted is considered poor), we swap the key present in the bucket with the new key. The old key is again reinserted using the same logic.\nCheck out this pdf to read more in-depth about Robin Hood hashing.\nCuckoo Hashing Working on Cuckoo Hashing is similar to the behavior of the Cuckoo bird, which lays its eggs in other birds\u0026rsquo; nests and, when those eggs hatch, throws out the eggs of the other birds. Similarly, here, an element occupies the slot of another element and throws out the old element.\nIt consists of multiple hash tables, each with its own hash function.\nInsertions: When inserting an element, the indexes of all the hash tables are calculated using their corresponding hash functions. If any of the buckets is empty, the new element is inserted. If not, one of the buckets is randomly selected, the new element replaces the old element, and the kicked-out element is reinserted into the hash table using the same logic. To prevent infinite loops, it allows only a limited number of reinsertions.\nDynamic Hashing Techniques Chained Hashing Chained hashing is the simplest form of hashing technique, in which a linked list is used.\nInsertions: When inserting a value, if the bucket to which the key is hashed is already full and the pointer stored is null, a new overflow bucket is allocated, and a pointer to this new bucket is stored in the old bucket that was full. Now, the new value is stored in the newly created bucket. And if there is a pointer to another bucket, and if a slot is empty in it, the value is inserted there. If even that bucket is full, and there is a pointer stored for another bucket, the other bucket is checked, and so on.\nLinear Hashing Linear hashing performs incremental growth of the hash table, where the hash table is not directly doubled in size, but a new bucket is added when an overflow occurs. It uses two hash functions and a pointer known as the split pointer to keep track of which bucket to split next.\nInsertions: Here, just like every other algorithm, we hash the key and find its hash bucket. If it is empty, we insert it there. If it is not, a new overflow bucket is added. Keep in mind that the overflow bucket can be present in linear hashing, but the number of overflow buckets is generally reduced as the algorithm proceeds. After the new overflow bucket has been added, another new bucket is added to the hash table, and a new hash function is used, say h2. Now, the values in the bucket to which the split pointer is pointing are rehashed again using the new h2 hash function, and the split pointer is made to point to the bucket.\nLookups: Lookups are a bit more complex here. Say we want to look up a value; we first hash it using the original hash function h1. If the indexed bucket is present before the split pointer, we rehash it again using the h2 hash function to get the bucket in which it is stored. And if the hashed bucket is present after the split pointer, we use the original hash function itself, so no rehashing is required.\nThe linear hashing performs splitting in a round-robin fashion and performs the splitting in the form of rounds. When a round is completed, the split pointer is reset to the 0th bucket. A round is set to be completed if all the initial buckets (those buckets that are present before the expansion) are split.\nExtensible Hashing Extensible hashing is a dynamic hashing method in which the table is grown and shrunk as requests are made. Usually, the size of the hash table is doubled when growing and halved when being shrunk. Note that even if the size of the hash table is doubled, buckets are not assigned yet, and new buckets are only created when a key is hashed to it. They are only allocated when they are needed. The method uses the first n bits of a hashed value to find the index of the key. Here, the number of first n bits used is called the count. A global count is stored for the entire hash table, and every bucket has its own local count.\nInsertions: Here, the first n bits of the value generated from hashing the key are used as the index. If there is an empty slot in the bucket, it is inserted there. If not, we first check if we want to increase the size of the hash table. This is done by checking the local count of the bucket and the global count. If both are the same or the local count is greater than the global count, then there is only one element in the hash table that points to the bucket. So, we increase the size of the hash table. We increment the global count by one (n + 1, so now the first n+1 bits are considered), effectively doubling the size of the hash table. For every two entries in the hash table, we make them point to their original bucket, and for the bucket where the overflow occurred, we create a new bucket and assign the second entry to point to the newly created bucket. Then, we retry the insertion, which will now be mostly successful.\nIf growing the hash table is not required, we just create a new bucket and assign the second entry pointing to the same bucket to point to the newly created bucket. Then, we retry the insertion, which will now be mostly successful. However, if all the values present in the bucket have the same search key, splitting will not solve the problem, so we must have some kind of way to handle large amounts of duplicate search keys.\n","permalink":"http://localhost:1313/posts/buffer-pool-manager-and-hashing/","summary":"This blog is a continuation of the database systems series, covering Lectures 5 and 6 of the Database Systems CMU 15-445/645 course.\nBuffer Pool The buffer pool is the memory present in the main memory (RAM) that is used to store pages that are most frequently used, similar to a cache. It is managed by the DBMS (buffer pool manager), because the DBMS can do a better job of managing pages than the OS.","title":"Buffer Pool and Hashing"},{"content":"This blog is a continuation of the database systems series, covering Lectures 3 and 4 of the Database Systems CMU 15-445/645 course.\nHow does a DBMS store data? A DBMS uses disks for data storage and persistence, and there are many storage devices that the database can use to store data.\nStorage Devices Storage devices are devices that are used to store data. There are mainly two types of storage devices: volatile and non-volatile.\nVolatile Storage Devices These devices are usually more expensive, faster, and have less storage capacity. Volatile storage devices require a constant supply of electricity to hold data. If the power is down, the data is completely lost.\nDevices categorized as volatile storage devices include CPU Registers, CPU Cache, and RAM.\nNon-Volatile Storage Devices Non-volatile storage devices are usually less expensive and provide far more storage capacity than volatile storage devices but are very slow. These storage devices do not require a constant supply of electricity, meaning the data remains even when the power is off.\nDevices categorized as non-volatile storage devices include SSDs, HDDs, DVDs, etc.\nHow does a DBMS store data on disks? A DBMS stores databases in a single file or multiple files with hierarchical relationships, or it can use one file per relation, etc.\nHow are files structured? Each file is divided into a bunch of fixed-sized chunks known as pages. The size of these pages varies from one DBMS to another. Storage is also divided into pages known as hardware pages, where the storage device can only guarantee an atomic write of a size equal to that of hardware pages. For instance, if the hardware page size is 4KB, then 4KB of data is written all at once or not written at all. There is no intermediate state.\nHow are pages structured? There are several ways to structure a page, depending on whether the size of the records to be stored is fixed or variable. Records are stored in pages. For simplicity, the size of the record is generally limited to that of the page, and a record does not span multiple pages.\nHow are pages structured if the size of the records is fixed?\nRecords can be stored sequentially, one after another, just like an array. When the size of the records is not a multiple of the page size, the last part of the page is left unused to ensure that the record stays in one page instead of spanning multiple pages.\nInsertion can be costly as one must parse the entire page to find an empty location. To handle this, the first empty record stores a pointer to the next empty record, and so on, forming a linked list of empty records known as a free list.\nHow are pages structured if the size of the records is variable?\nThe main problem with variable-sized records is that maintaining the free list is not helpful as the size of the record can change from one to another. If the size of an empty record found is 50 bytes and we want to insert a record of 56 bytes, which is not possible, to handle this slotted pages are used.\nIn variable-length records, the records have two parts. The first part has a fixed length and is the same for all the records of the same relation. The second part is the variable-length data.\nThe first part of the record consists of many pairs of values, where each pair stores the starting address and length of each attribute’s data. In pages, slots are used to track the records. A slot merely stores the starting and ending address of a record. These slots are present at the start of the page and grow towards the end of the page. Here, records are stored at the end and grow towards the start of the page. When both the slots and pages meet, we know that the page is full.\nLog structure of pages\nIn this form, relations are stored as logs. For example:\nINSERT A; INSERT B; INSERT C; UPDATE A; DELETE B; Here, writing to the database is extremely fast, but reading is very slow because one must parse the entire log to get the desired record.\nGenerally, logs are added from start to end, and reading is done from end to start.\nReading from end to start can be helpful. Say we want to read record B. If we were reading from start to end, we would have to parse the entire file, but if we read from end to start, we can stop parsing as soon as we find the INSERT B; log.\nHow Are Tuples/Records Structured? Tuples are simply a sequence of bytes that make sense to the DBMS. They generally contain:\nTuple header:\nThe tuple header stores metadata related to the tuple, such as the number of null attributes, the size of the tuple, the size of the attributes, etc.\nTuple Data:\nThe actual data for the attributes. These attributes are generally stored in the order specified in the table. For instance, if the table is defined as follows:\nCREATE TABLE table_name ( column1 datatype, column2 datatype, column3 datatype, .... ); Then tuples will be stored in the following order:\nUnique Identifier: Each tuple in the database is assigned a unique identifier. The most common is a combination of page ID and an offset or slot.\nWhat If the Size of a Tuple Is Greater Than the Size of a Page?\nSometimes, we need to store large files like images or videos. In cases where the size of a record exceeds the page size, the DBMS uses special pages known as overflow pages, and a pointer to the overflow page is stored in the record. If the data does not fit in a single record, it can span multiple overflow pages, or it can be stored in an entirely new file outside the database. A pointer to that data file is then stored in the record. However, some features offered by the DBMS may not apply to this external data file.\nHow are records organized in files? So, how does the DBMS know which block to store the record in? There are multiple ways to organize records in files:\nHeap File Organization:\nIn heap file organization, records are stored in any available free space in the files, and there is no specific ordering of the records. Once stored, they are generally not moved. To find free space quickly, the DBMS uses an array of bytes where each byte corresponds to a page and stores a number representing the fraction of space that is free in that page. For example, if the page size is 8 bytes and the value stored is 6, then a 6/8th fraction of space is free in that page. This array of bytes is known as the free space map. Sequential File Organization:\nIn sequential file organization, records are stored in a specific order based on a search key. Sequential file organization is useful for displaying and computing queries. Each record stores a pointer to the next record, which comes after it sequentially. Here, the records are also stored as physically close as possible to reduce disk read time. For instance, say we have a relation stored in sequential file organization, and the search key is Id. If we want to insert a new record, \u0026ldquo;D,\u0026rdquo; whose Id is 4, the DBMS first locates the record that comes before Id 4, which is 3. if there is free space in the same block 4 is stored there; if not, 4 is stored as a new overflow block, and 3 points to that overflow block. Multitable Clustering File Organization: In multitable clustering file organization, records belonging to different relations are stored on a single page or block. This form of file organization helps when handling JOIN queries. However, reading records that belong to a single relation but are stored in multitable clustering can be slow, as the records will be spaced far apart. B+ Trees File Organization: B+ tree file organization uses a B-tree data structure to organize files. Here, the B+ tree provides very fast insert, read, delete, and update operations. The time complexity of B+ trees is O(log(n)) for both insertion and deletion. B+ trees are like evolved versions of binary trees. Hashing file organization In hashing file organization, a key is hashed to determine the location where the record should be stored. Hashing allows for both faster insertions and faster lookups, but the hash function used must be fast and distribute data uniformly. Hashing file organization is not suitable for OLAP-type workloads. Why Does the DBMS Manage Memory Better Than the OS? The DBMS can rely on the OS to load pages into memory when required. While this can be helpful, the DBMS can handle memory much better because it knows much more about the database files than the OS does. The DBMS can use its knowledge of database files to prefetch pages, search for pages more effectively, and so forth.\nIf the DBMS relies on the OS for managing memory, it can be dangerous because if any changes are made to a page, the OS can remove the page from memory without writing it back to disk, causing data loss since the DBMS has no control over the OS page cache.\nThus, many DBMSs have their own Buffer Pool that manages the loading of data to and from disk and memory. If an execution engine requests a specific page to execute a query, it makes a request to the Buffer Pool, which then loads the required page into memory and returns a pointer to it.\nSystem Catalog The DBMS needs to store information such as relations present in a database and files associated with the databases. The DBMS maintains a catalog that stores all the required data. Some additional data, such as the number of records, distinct records, and the sum of attributes, can also be stored using the DBMS\u0026rsquo;s own functionalities. However, metadata required for the catalog itself (a catalog of catalogs 😅) is also embedded in the software.\nWorkloads Workloads refer to the general types of requests that the DBMS should process.\nThere are several different types of workloads, including Online Analytical Processing (OLAP), Online Transactional Processing (OLTP), Decision Support System (DSS), and Business Intelligence (BI).\nThe three main types of workloads are:\nOLTP(online transactional processing) Most of the query requests received by the database are short and mostly require accessing only one record. They usually involve insert, delete, and update operations. The time taken to process a request is minimal. OLAP(online analytical processing) Most of the query requests involve accessing most or all of the records in the table. These requests are usually analytical in nature and can be used to generate reports, analyze trends, etc. They usually take a long time to process, often hours. HTAP(Hybrid transaction + analytical processing) Most queries received by the DBMS are usually a mix of both OLAP and OLTP queries. Storing Records Based on DBMS Workload N-Ary Storage Model (NSM) In this storage model, the records are stored normally, with a record containing the data of all attributes. This type of storage model is usually helpful for OLTP workloads, as the entire record can be modified in one place. However, it is not preferred for OLAP workloads because if a query requires only a few attributes of a relation, the entire page is loaded into memory, which contains attributes that are not needed. This wastes buffer pool memory and disk time (more time is taken to load).\nDecomposition Storage Model (DSM) In this storage model, only specific attributes are stored on a page. For example, if a relation has attributes A, B, and C, all values of attribute A are stored sequentially on one page, B on another, and so forth. So, when a query needs specific attributes of a relation, only those can be loaded into the buffer pool.\nThis storage model is suitable for OLAP workloads but not for OLTP, as all attributes of a record are stored on different pages. If we need to update a single record, we must update multiple pages.\nThe DBMS can use a fixed-length approach, where all attributes of the same record are stored at the same offset. So, if we want to access an attribute of a record, we can jump to that specific offset.\nAnother approach is the embedded tuple approach, where every tuple is embedded with a primary key of the corresponding tuple. This wastes memory by storing primary keys and is not widely used.\n","permalink":"http://localhost:1313/posts/database-storage/","summary":"This blog is a continuation of the database systems series, covering Lectures 3 and 4 of the Database Systems CMU 15-445/645 course.\nHow does a DBMS store data? A DBMS uses disks for data storage and persistence, and there are many storage devices that the database can use to store data.\nStorage Devices Storage devices are devices that are used to store data. There are mainly two types of storage devices: volatile and non-volatile.","title":"Database Storage"},{"content":"I am currently studying database systems. This blog summarises lectures 1 and 2, as well as the SQL homework from the Database Systems CMU 15-445/645 course.\nIt\u0026rsquo;s fascinating to realize the extent to which we interact with database systems in our daily lives. They\u0026rsquo;re not just confined to large-scale applications but are also embedded in our browsers and mobile devices. Before the advent of database management systems, data management was complex, with a strong coupling between the data and its management. This required constant re-implementation for different applications.\nTo address this issue, Edgar F. Codd introduced the concept of the relational model, one of many data models including graph, network, document, and arrays/matrices.\nA data model is a collection of concepts for describing the data in a database. The relational model is an example of a data model.\nThe most widely used database system is the relational database, which was developed based on Edgar F. Codd\u0026rsquo;s proposals. In this system, data is stored in a relation (or table), and each record (row) or tuple contains the data. A relation consists of attributes (columns).\nSo, how do we communicate with a DBMS?\nWe use Data Manipulation Languages (DMLs) which can be procedural or non-procedural (declarative). Procedural languages specify both what we want and how we want the data, while non-procedural languages only specify what we want, leaving the retrieval method up to the DBMS.\nRelational algebra, is like a math of databases and uses special symbols to represent specific operations on relations. Each operation takes one or more relations as input and produces a new relation as output.\nThere are several types of operations in relational algebra:\nSelect: This operation takes a relation and outputs a subset of tuples that satisfy a certain condition or predicate. Projection: It outputs a relation with tuples containing only specified attributes. Union: This operation outputs a relation containing all tuples appearing in at least one of the input relations. Intersection: It outputs a relation containing all tuples appearing in both input relations. Difference: This operation outputs a relation containing all tuples appearing in the first relation but not the second. Product: It outputs a relation containing all possible combinations of tuples from the input relations. Join: This operation outputs a relation where two relations are joined on a specific attribute. SQL is a non-procedural language, that is we only say what we want, and how? is managed by the database, there are many SQL database systems such as postgreSQL, MySQL, sqlite, all the SQL database are mostly similar but there is a difference in the methods offered and how the database system manages different types, a database system is said to support SQL if it implements SQL 92,\nhere are my answers for the assignment along with some expiation\nQ1\nSELECT CategoryName FROM Category ORDER BY CategoryName; -- this command just selects the CategoryName attribute from Category Relation -- and they tuples are ordered by the category name in ascending order Q2\nSELECT DISTINCT ShipName, SUBSTR(ShipName, 0, INSTR(ShipName, \u0026#39;-\u0026#39;)) FROM \u0026#34;Order\u0026#34; AS o WHERE o.ShipName LIKE \u0026#34;%-%\u0026#34; ORDER BY o.ShipName; -- this command selects shipName attribute and -- all the characters preceding hypen \u0026#39;-\u0026#39;, this is achived -- by using the INSTR string function, which can be used to -- get the position of a character in a string, -- and SUBSTR function is used to get a substring of a string -- LIKE is used for wild card matching of string, where \u0026#39;%\u0026#39; any characters -- including an empty string \u0026#39;\u0026#39; -- the retreived tuples are ordred by the shipName Q3\nSELECT Id, ShipCountry, CASE WHEN (ShipCountry LIKE \u0026#39;USA\u0026#39;) OR (ShipCountry LIKE \u0026#39;Mexico\u0026#39;) OR (ShipCountry LIKE \u0026#39;Canada\u0026#39;) THEN \u0026#39;NorthAmerica\u0026#39; ELSE \u0026#39;OtherPlace\u0026#39; END FROM \u0026#34;Order\u0026#34; WHERE Id \u0026gt;= 15445 ORDER BY Id LIMIT 20 -- this command selects the Id and ShipCountry attribute from \u0026#34;Order\u0026#34; relation -- CASE is similar to if else in general progrmming, it is used to check if the -- ShipCountry is \u0026#39;USA\u0026#39; or \u0026#39;Mexico\u0026#39; or \u0026#39;Cannada\u0026#39;, if it is then the value \u0026#39;NorthAmerica\u0026#39; -- is stored in tuple else \u0026#39;OtherPlace\u0026#39; is stored in the tuple -- again ORDER BY is used to order the typle according to their Id -- WHERE Id \u0026gt;= 15445 is used to select only the tuples having Id more then 15445 -- LIMIT is used to limit the output to only 20 tuples(rows) Q4\nWITH orderDelay (ShipId, delay) AS ( SELECT ShipVia, CASE WHEN ShippedDate \u0026gt; RequiredDate THEN \u0026#39;late\u0026#39; ELSE \u0026#39;ontime\u0026#39; END AS delay FROM \u0026#34;Order\u0026#34; ) SELECT CompanyName, ROUND( 100.0 * COUNT( CASE WHEN od.delay = \u0026#39;late\u0026#39; THEN 1 ELSE NULL END ) / COUNT(*), 2 ) AS delayPercentage FROM orderDelay AS od, Shipper WHERE od.ShipId = Shipper.Id GROUP BY ShipId -- WITH AS statement is used to create an temprovary table -- which has ShipVia attribute stored as ShipId and it is calcuated if the -- order was late using ShippedDate and RequiredDate attributes and stored -- as delay attribute in orderDelay table -- were are SELECTing CompanyName and calculating the percentage of late orders -- and rounding them to 2 decimal places using ROUND function and storing it as -- delayPercentage attribute, we are using both the Shipper, and the newly created -- orderDelay Relation to calculate the delayPercentage Q5\nSELECT CategoryName, COUNT(*) AS numberOfProducts, ROUND(AVG(UnitPrice), 2), MIN(UnitPrice), MAX(UnitPrice), SUM(UnitsOnOrder) FROM \u0026#34;Product\u0026#34; AS p JOIN \u0026#34;Category\u0026#34; AS c ON p.CategoryId = c.Id GROUP BY c.Id -- here we are joining two relations \u0026#34;Product\u0026#34; and \u0026#34;Category\u0026#34; based on the CategoryId -- and calculating umberOfProducts using COUNT, average UnitPrice using AVG, minimum -- UnitPrice using MIN and maximum UnitPrice using MAX and total orders using SUM -- aggrigate functions, -- here the GROUP BY is used to group the tuples having same CategoryId, hence the -- aggrigate function performs operations on the gorups of tuples having same -- CategoryId Q6\nWITH discounted (orderId, productId) AS ( SELECT OrderId, ProductId FROM OrderDetail as od, Product as p WHERE od.ProductId = p.Id AND p.Discontinued = 1 ) SELECT ProductName, CompanyName, ContactName FROM ( SELECT *, RANK() OVER( PARTITION BY d.ProductId ORDER BY o.OrderDate ) as rank FROM \u0026#34;Order\u0026#34; as o, discounted as d WHERE o.id = d.orderId ) JOIN Customer ON CustomerId = Customer.Id JOIN Product ON Product.id = ProductId WHERE rank = 1 ORDER BY ProductName -- here we are getting a relation of all the products, which are discounted -- and storing the obtained tuples in discounted relation -- we use inner Query to add a new attribute of rank based on -- when the companies have ordered the product, this is acheived -- using the RANK window function and PARTITION BY and ORDER BY -- so tuple having RANK 1 ordered the product first -- finally in the outer query we only select the companies which -- brough the product first Q7\nSELECT o.Id, OrderDate, LAG(OrderDate, 1, 0) OVER( ORDER BY OrderDate ) AS PreviousOrderDate, ROUND( julianday(OrderDate) - julianday( LAG(OrderDate, 1, 0) OVER( ORDER BY OrderDate ) ), 2 ) AS \u0026#39;Lag\u0026#39; FROM \u0026#34;Order\u0026#34; AS o, Customer AS c WHERE o.CustomerId = c.Id AND o.CustomerId = \u0026#39;BLONP\u0026#39; ORDER BY OrderDate LIMIT 10 -- here we mainly use the LAG function to get the value of the -- preceding tuple, LAG should almost always be used with ORDER BY -- becuase if we didn\u0026#39;t use the ORDER BY the ouput can be randomized -- we are caclucating the time difference between two orders using -- the LAG aggrigate function and julianday function as ouputing it as -- \u0026#39;LAG\u0026#39; attribute -- we are limiting the output to 10 tuples using LIMIT Q8\nWITH expendeture(Id, expen) AS ( SELECT CustomerId as Id, SUM(UnitPrice * Quantity) AS expen FROM \u0026#34;Order\u0026#34; AS o JOIN OrderDetail AS od ON o.Id = od.OrderId GROUP BY CustomerId ) SELECT CASE WHEN ( e.Id NOT IN ( SELECT Id FROM Customer ) ) THEN \u0026#39;MISSING_NAME\u0026#39; ELSE ( SELECT CompanyName FROM Customer WHERE e.Id = Id ) END AS \u0026#39;CompanyName\u0026#39;, e.Id AS \u0026#39;CustomerId\u0026#39;, ROUND(e.expen, 2) AS \u0026#39;Expences\u0026#39; FROM expendeture AS e ORDER BY e.expen -- here we create a new table which contains the customerId and -- customer expences as attributes then in the outer query -- we are using case to check if the customer in present in the customer table -- if present we use the CompanyName else we use \u0026#39;MISSING_NAME\u0026#39; -- we round the expences of company to -- two decimal places using ROUND function Q9\nselect RegionDescription, FirstName, LastName, BirthDate FROM ( select *, RANK() OVER( PARTITION BY r.id ORDER BY BirthDate DESC ) AS rank from Employee as e JOIN EmployeeTerritory as et ON e.id = et.EmployeeId JOIN Territory as t ON et.TerritoryId = t.Id JOIN Region as r ON t.RegionId = r.Id ) WHERE rank = 1 GROUP BY Id ORDER BY RegionId -- here we are using the inner query, to rank the employee -- according to their birthdate -- and in the outer query we are selecting the employee whose rank is 1 -- and ordering the output by RegionId Q10\nWITH productNames(pname, id) AS ( SELECT ProductName, ROW_NUMBER() OVER( ORDER BY ProductId ) FROM \u0026#34;Order\u0026#34; as o JOIN OrderDetail as od ON o.id = od.OrderId JOIN Product as p ON od.ProductId = p.Id JOIN Customer as c ON c.id = o.CustomerId WHERE julianday(substr(OrderDate, 0, instr(OrderDate, \u0026#34; \u0026#34;))) = julianday(\u0026#39;2014-12-25\u0026#39;) AND CompanyName = \u0026#39;Queen Cozinha\u0026#39; ORDER BY ProductId ) SELECT * FROM productNames -- here we are using julianday to get the day -- order which are made on the required day -- and were are selecting only the required companyName -- and ordering the orders according to their productId i am still trying to wrap my head around the recursion in SQL, so my 10th answer is incomplete,\nhere are some sources that help me complete this assignment\nhttps://www.sqlitetutorial.net/\nhttps://www.youtube.com/@AlexTheAnalyst\n","permalink":"http://localhost:1313/posts/database-systems-1/","summary":"I am currently studying database systems. This blog summarises lectures 1 and 2, as well as the SQL homework from the Database Systems CMU 15-445/645 course.\nIt\u0026rsquo;s fascinating to realize the extent to which we interact with database systems in our daily lives. They\u0026rsquo;re not just confined to large-scale applications but are also embedded in our browsers and mobile devices. Before the advent of database management systems, data management was complex, with a strong coupling between the data and its management.","title":"Intro to Database Systems"},{"content":"What is a webhook?\nA webhook is an HTTP POST request that is made when a change or event happens in a system. For example, this could occur when the system starts, stops, or when certain conditions are met. This HTTP POST request contains information regarding the change, which programmers can use to initiate actions.\nConsider GitHub as an example. When a push event occurs (which is a webhook event), GitHub sends a POST request to the payload URL. This request contains the required information.\nWhat is a webhook tester?\nAs you saw with GitHub, it needs a \u0026ldquo;payload URL\u0026rdquo; to send the POST request to. This payload URL must be publicly available. If you\u0026rsquo;re building a program that receives POST requests and performs actions based on them, it needs to be able to receive the request first. To achieve this, you\u0026rsquo;d typically need a publicly available URL. However, if you\u0026rsquo;re just starting development, you might not want to purchase your own domain. This is where a webhook tester comes in. A webhook tester provides a temporary URL that you can use during development, eliminating the need to set up your own domain.\nWebhook Tester Design\nI am building a webhook tester using Golang. Here\u0026rsquo;s an overview of how it works:\nThere are two main parts: a server and a client, referred to as whserver and whclient, respectively. The whclient program is used by someone who is developing a program (program A) that performs an action when it receives a webhook POST request. Whclient is a binary file that runs on a client machine. It takes the port number as an argument, which is the port on which program A is running. When the client runs the binary file, it generates a random URL that is publicly available. This URL can be used as a payload URL to which the POST request can be made. The whserver listens for the POST request. Upon receiving it, the server encodes the request and transmits it to the whclient through a WebSocket connection. This WebSocket connection is established when the whclient binary file is initially run. The whclient rebuilds the request and makes the same request to the locally running program, simulating as if program A received the request directly. How it Works\nThere are three main packages: CLI (command-line interface), server, and serialize.\nServer Package\nThe server has two main functions:\nForwarding the received HTTP POST request to the corresponding client. Adding new clients. Adding New Clients\nThis is done using a type called subdomains, which is a map that maps a URL to an HTTP handler function. The key is the URL, and the value is the handler function.\nWhen the client binary makes a request to the /ws endpoint, the NewWebHookHandler is used to create a new mux (multiplexer). When a new request is received at the /ws endpoint, a new WebSocket connection is created between the client and the server, and a new random URL is generated. Then, the AddNewClient(u string, ws *websocket.Conn) method is called. Here, u is the randomly generated URL, and ws is a pointer to the established WebSocket connection.\nThe AddNewClient function creates a new handler function that uses the ws connection to send messages to the client. It also creates a new entry in subdomains. This establishes a WebSocket connection with the client, associates a randomly generated URL with it, and creates a handler function to communicate with the client.\nForwarding Messages to Clients\nWhen the server receives any POST request at the root endpoint, it calls the ServeHTTP function, which is a method of subdomains. This gives the server access to the URLs and corresponding client handlers. The ServeHTTP method of the subdomains type iterates through the URLs one by one and checks if the received request\u0026rsquo;s URL matches any of them. If a match is found, it calls the corresponding handler function, which encodes the received request and sends it to the client through the WebSocket connection. This way, the client receives the HTTP request.\nCLI Package\nThe client has two main purposes:\nEstablishing a WebSocket connection with the server and displaying the URL received from the server. Making the same request to the locally running program. The client is primarily built around the client structure.\nClient Structure\ntype client struct { URL string Conn *websocket.Conn httpClient *http.Client } The client structure defines the following fields:\nURL: This string stores the URL generated by the server. Conn: This pointer holds the established WebSocket connection with the server. httpClient: This is used to make HTTP requests to the local program. Client Functionality\nThe client establishes a WebSocket connection with the server by calling the NewConn function:\nfunc NewConn(wsLink string) *websocket.Conn { ws, _, err := websocket.DefaultDialer.Dial(wsLink, nil) if err != nil { log.Fatalf(\u0026#34;error establishing websocket connection: %v\u0026#34;, err.Error()) } return ws } This function attempts to connect to the provided URL (wsLink) and returns a pointer to the established WebSocket connection (ws). If an error occurs during connection establishment, it\u0026rsquo;s logged using log.Fatalf.\nOnce connected, the client retrieves the generated URL from the WebSocket connection and creates a new http.Client instance. Finally, it enters a loop to listen for incoming messages on the WebSocket connection using the Listen method:\nfunc (c *client) Listen(w io.Writer, fields []string, urlstr string) error { data, msgType, err := read(c.Conn) if err != nil { return err } if msgType == websocket.TextMessage { fmt.Fprint(w, \u0026#34;\\n\u0026#34;+string(data)) } else if msgType == websocket.BinaryMessage { req := serialize.DecodeRequest(data) fmt.Fprint(w, readRequestFields(fields, *req)) req.URL, _ = url.Parse(urlstr) req.RequestURI = \u0026#34;\u0026#34; _, err := c.httpClient.Do(req) // Send the decoded request to the local program if err != nil { log.Fatalf(\u0026#34;\\ncli could not forward message to local server: %v\u0026#34;, err) } } return nil } The Listen method first reads data, message type, and any potential errors from the connection. It then handles different message types, on receiving a binary message it decodes it, and makes the same request to the locally running program\n","permalink":"http://localhost:1313/posts/webhook-tester/","summary":"What is a webhook?\nA webhook is an HTTP POST request that is made when a change or event happens in a system. For example, this could occur when the system starts, stops, or when certain conditions are met. This HTTP POST request contains information regarding the change, which programmers can use to initiate actions.\nConsider GitHub as an example. When a push event occurs (which is a webhook event), GitHub sends a POST request to the payload URL.","title":"WebHook Tester"},{"content":"TCP stands for Transmission Control Protocol. It\u0026rsquo;s built upon the IP layer and provides features like reliable packet transmission, flow control, congestion control, and congestion avoidance.\nWhy is TCP the most used protocol?\nTCP is widely used because of the features it offers, such as reliable packet transmission, flow control, and others. Reliable transmission ensures data arrives complete and in order, making it crucial for applications like file transfers, email, and web browsing.\nWhat does reliable packet transmission mean?\nWhen packets travel over the internet, they can get lost or corrupted. Reliable transmission guarantees that all packets arrive at their destination complete and in the correct order. If a packet is lost or corrupted, TCP detects it using checksums and requests the sender to resend it. This ensures accurate and complete data exchange.\nWhat is flow control?\nThink of flow control like regulating water flow from a large tank to a small cup. If the water flows too fast, the cup overflows. Similarly, Our computer will have some memory buffer which is used to store the incoming packers before processing, when the server keeps on sending the data and when our ability process them in less, packets will start accumulating in the memory buffer and soon will lead to packet loss due to buffer overflow, to avoid this TCP will keep track of a variable known as receive window (rwnd) both the server and client will have their own receive window, and these are initially exchange during the three-way handshake, when there is a change in rwnd it can again be exchanged between the server and client to keep up with the new requirements\nWhat is congestion control?\nTwo main factors determine your internet speed: bandwidth and receive window. Flow control takes care of the receive window, but what about bandwidth? Imagine you have a 30Mbps connection and the server sends data at 100Mbps. This creates network congestion because your connection can only handle 30Mbps, leading to packet loss. So, how does the server handle this?\nThe server tracks another variable called \u0026ldquo;congestion window size (cwnd)\u0026rdquo; – the amount of data it can send at once. Unlike the receive window, the cwnd is not shared, and for Linux systems, it starts at 4kb. TCP dynamically adjusts the cwnd based on acknowledgments (ACKs) from the client. It increases the cwnd if all packets are received and decreases it if there\u0026rsquo;s any packet loss. There are various congestion control algorithms like Slow Start, Congestion Avoidance, and Fast Retransmit that utilize ACKs to determine the optimal cwnd value, ensuring efficient data transfer without overwhelming the network.\nBandwidth Delay Product (BDP)\nIt ****is the product of a connection\u0026rsquo;s bandwidth and its end-to-end delay (also known as round-trip time). It indicates the maximum amount of unacknowledged data that can be in transit without overwhelming the network or receiver\u0026rsquo;s buffers.\nThe maximum amount of unacknowledged data in transit is limited to the minimum of the receive window size (RWND) and congestion window size (CWND). This is because:\nIf CWND were greater than RWND, the sender might transmit more data than the receiver can handle, leading to buffer overflow and packet loss. If RWND were greater than CWND, the sender might not fully utilize available bandwidth, potentially leading to underutilised links. therefor even if we have sufficient bandwidth our internet speed can be limited due to our receive window\n","permalink":"http://localhost:1313/posts/tcp/","summary":"TCP stands for Transmission Control Protocol. It\u0026rsquo;s built upon the IP layer and provides features like reliable packet transmission, flow control, congestion control, and congestion avoidance.\nWhy is TCP the most used protocol?\nTCP is widely used because of the features it offers, such as reliable packet transmission, flow control, and others. Reliable transmission ensures data arrives complete and in order, making it crucial for applications like file transfers, email, and web browsing.","title":"TCP"},{"content":"How information is sent across the internet? Internet is a network of networks, billions of computers interconnect make the internet, information is sent across the internet in the form of packets, packets can be thought of a little containers of data, if you want to share an image to your friend, first the large image is broken down into number of packets, and these packets are transferred to your friend’s computer, in your friends computer they are reordered and assembled together(not necessarily) to form the image\nWhat are packets? Packets in the internet closely resembles the a post in real life, just like the letter in the post they have some digital data to carry and like the to and from addresses on the envelope, packets have an IP Header, IP Header is like the address of the houses, it contains source and destination IP of the computers along with other stuff\nHow do these packets actually travel? Packets travel from hopping from one router to another, when a package is sent it fist goes to router which is provided by ISP(internet service provider) to which we are connected to, and then it goes from one router to another, how does the router decide on which direction to send the packet?, router does that from looking at the packet header, which contains the destination IP address, then it looks at a table(routing table), table contains directions and corresponding destination IP address, and now it directs the packets in the correct direction, the router also considers other factors such as network traffic etc\nWhat is a protocol? Protocols are a set of rules that are agreed upon by the computers, which facilitate the communication between them, just like how two persons should know the rules of the language to communicate in that language.\nThere are many protocols some of them are IP, TCP, UDP, HTTP\nLets take about each of them now\nIP (internet protocol): IP stands for internet protocol, IP can be thought of as a set of rules by which the packets are sent and received over the internet, all the devices which use IP has a unique IP address which is used by other devices to identify that device and also by routers to direct the packets to it. IP takes care of identifying the devices, routing the packets and breaking down and reassembling the packets into larger files\nTCP (transmission control protocol): TCP operates on a layer above IP, TCP takes care of the things like inorder delivery of the packets, TCP maintains a reliable communication, that TCP makes sure that all the packets are received and they are received in order, TCP makes request to the sender for the missing packets if there are any, TCP also takes care of the thing like flow control and congestion control, flow control and congestion control govern the rate of transmission of packets.\nUDP (user datagram protocol): UDP is used if we want to make faster communication, UDP doesn’t guarantee the packet delivery and also doesn’t deliver the packets in order, but transmission is way faster then TCP and is generally used for online game, live streaming and voice calls, the terms datagrams and protocols are used interchangeably but the key difference is that we use the terms datagram when there is un guaranteed delivery of the packet\nHTTP (hypertext transfer protocol): HTTP is a that governs the data transfer, it is primary used to transfer text data but images, audio and videos can also be transferred, it is based on response request model, where client makes a request to the server and the server responds back, it provides different methods for different types of request for example GET to request data, POST to send the data, PUT to change the existing data\n","permalink":"http://localhost:1313/posts/internet/","summary":"How information is sent across the internet? Internet is a network of networks, billions of computers interconnect make the internet, information is sent across the internet in the form of packets, packets can be thought of a little containers of data, if you want to share an image to your friend, first the large image is broken down into number of packets, and these packets are transferred to your friend’s computer, in your friends computer they are reordered and assembled together(not necessarily) to form the image","title":"internet basics"},{"content":"event modelling What is event modeling?\nEvent modeling is a simple technique used to design a system. It generally takes relatively little time to learn and get started. In event modeling, we use sticky notes to write things down.\nSticky note colors and their purposes:\nOrange sticky notes: Represent events. We write down the event occurrence in the past tense. Events are generally triggered by the UI or by some external API. Examples: Position updated, request received, response sent Blue sticky notes: Represent commands. We write down the command as an assertive sentence. A command is something that modifies the system state and is caused to happen. Examples: Update player health, send request Green sticky notes: Represent views. A view is like a snapshot of the current system state. It can be used to display the current state of the system to the user and can also be used by other system applications. Examples: Display player\u0026rsquo;s health Let\u0026rsquo;s try to create an event modeling diagram for a simple fitness app that we will be making.\nLet\u0026rsquo;s start with what we want our app to do:\nDisplay today\u0026rsquo;s to-do exercises Send reminder notifications to exercise Keep track of streaks Now that we know what our simple app should do, let\u0026rsquo;s think about the events. Here are a few that I came up with:\nRequested today\u0026rsquo;s exercises Completed today\u0026rsquo;s exercises Alarm set Notification sent Requested current streak Set time reached Now that we have our events, let\u0026rsquo;s see how these events can be triggered. We use simple wireframes or actual screenshots of the app (if it\u0026rsquo;s already developed) to represent the UI. We align the UI wireframe with the event that will be triggered by user interaction.\nNow, let\u0026rsquo;s add the blue sticky notes for the commands that need to be executed in order to trigger the events. Commands generally act as inputs from the user that change the state of the system\nAnd now, let\u0026rsquo;s also include green sticky notes, which are generally used to display the state of the system. You can also use gears or other diagrams to represent work done by the API used by our app. Here, I have represented it as a cloud as a process.\nto know more about event modelling see: https://eventmodeling.org/posts/what-is-event-modeling/\nWhy we need to use event modelling?\ni feel like event modelling helps us understand the overall working of the system better\nthe idea can be communicated more effectively to the non technical people like UI\\UX designers and more people can contribute actively to the designing and features of the system\nevent modelling make building CQRS based systems easier as there is a clear separation between the query an view in the system\nWhat is CQRS?\nCQRS stands for Command Query Responsibility Segregation. It is a design pattern that separates the read model from the write model. This means using one model for querying data and a separate model for updating data (including creates, updates, and deletes).\nWhen should we use CQRS?\nCQRS is generally used when there is a nonuniform distribution between queries to the database and updates to the database. When we have a single model for basic CRUD (Create, Read, Update, Delete) operations, scaling is not very efficient. Having two different models helps us scale the query model and update model independently, which makes our application more efficient. Proper scaling can even be done if we use different databases for reads and updates, but we need to make sure that they are in sync.\nThe CQRS model is also used when the business logic becomes more complex for basic CRUD operations.\nThings to keep in mind before implementing CQRS:\nImplementing CQRS can get complex, and if we are using separate databases for reads and updates, we need to maintain consistency between the databases. It is preferred not to use CQRS when the business logic is simple and simple CRUD models do the job.\nsources:\nhttps://eventmodeling.org/posts/what-is-event-modeling/\nhttps://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs\nhttps://martinfowler.com/bliki/CQRS.html\n","permalink":"http://localhost:1313/posts/event-modelling/","summary":"event modelling What is event modeling?\nEvent modeling is a simple technique used to design a system. It generally takes relatively little time to learn and get started. In event modeling, we use sticky notes to write things down.\nSticky note colors and their purposes:\nOrange sticky notes: Represent events. We write down the event occurrence in the past tense. Events are generally triggered by the UI or by some external API.","title":"event modeling"},{"content":"Part 1: Introduction This is an article about my experience completing \u0026ldquo;Go with Tests.\u0026rdquo; I wanted to learn a back-end programming language for web development, so after completing CS50, I first got started with web development from The Odin Project. However, I only did a few lessons and wasn\u0026rsquo;t very consistent. Additionally, I felt like every three out of five engineering college students were learning the MERN tech stack, so I wanted to do something different.\nI first stumbled across Go from ThePrimeGen video, which led me to boot.dev, where I learned about Go. Intrigued by this mix of C and Python, I discovered the free GitBook \u0026ldquo;Learn Go with Tests\u0026rdquo; and decided to give it a serious try and actually complete it.\nIt\u0026rsquo;s funny that I got stuck in the first lesson itself. I had dual-booted my laptop and was using Ubuntu for programming, but I couldn\u0026rsquo;t install any packages. Even though the commands ran, I couldn\u0026rsquo;t see the files in the /bin directory. The solution was simply to change the permissions of the Go directory, but it took me quite a while to figure that out.\nBefore \u0026ldquo;Go with Tests,\u0026rdquo; I had zero knowledge of TDD or how software applications were built and deployed in the real world. The first few lessons, until maps, were quite straightforward. They covered basic syntax and built the habit of using TDD to solve problems. Did you know there are no while or do while loops in Go? That surprised me! Slices and interfaces were new concepts for me, so I needed to do some YouTube watching and Googling to understand them. LLMs like Bard and ChatGPT were also a big help. When I didn\u0026rsquo;t understand a concept or got stuck, I would ask them \u0026ldquo;Explain {topic} in simple terms.\u0026rdquo; Then, I would watch videos or read articles to try to understand the concepts and write a detailed message explaining what I didn\u0026rsquo;t understand and asking if my understanding was correct. This method helped me a lot, and I think you should give it a try.\nPart 2: Testing Fundamentals Things got a bit harder with the testing fundamentals. I had no idea what acceptance tests were, but after watching some videos and doing some back-and-forth with LLMs, I got to know them better. In short, they are tests written to check the application\u0026rsquo;s working from the user\u0026rsquo;s perspective. I also learned how Docker is used to test the application in different environments and about the different types of test doubles that can be used. Most of the chapters used HTTP in the lessons, which helped me better understand how Go works with HTTP.\nPart 3: Building an Application I think building an application was smooth sailing. Most of the things used were covered previously, and I was keeping track of things and resources using Notion. If I forgot something, I would go back and do a quick read. Even then, if I didn\u0026rsquo;t get it, I would rewatch the videos on that particular topic and go back. Here\u0026rsquo;s a repo that i used to keep track of things, and I\u0026rsquo;ve also linked my Notion notes for some of the chapters. Take a look if you like!\nOverall, I feel like \u0026ldquo;Learn Go with Tests\u0026rdquo; was a pretty good experience and helped me pick up a lot of new things, not just language-specific but programming-related in general. It was challenging enough to keep me engaged but not so hard that I wanted to give up. If you\u0026rsquo;re a beginner-intermediate programmer, I think you should definitely give it a try.\ngithub repo used to track progress: https://github.com/sanjayJ369/learningGo\nmy github repo: https://github.com/sanjayJ369\n","permalink":"http://localhost:1313/posts/learn-go-with-tests/","summary":"Part 1: Introduction This is an article about my experience completing \u0026ldquo;Go with Tests.\u0026rdquo; I wanted to learn a back-end programming language for web development, so after completing CS50, I first got started with web development from The Odin Project. However, I only did a few lessons and wasn\u0026rsquo;t very consistent. Additionally, I felt like every three out of five engineering college students were learning the MERN tech stack, so I wanted to do something different.","title":"learn go with tests"}]