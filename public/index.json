[{"content":"Parts of GFS GFS (Google File System) is a reliable distributed file system that is built on top of unreliable commodity machines.\nCommodity machines break down, and they break down pretty often, and given that there are many such machines, at any given moment it is a miracle if all the machines are working fine.\nSo, it must constantly monitor itself and detect, tolerate, and recover from component failures regularly.\nSystem Requirements Building Blocks : As GFS is a file system that is built on top of commodity machines which fail a lot\u0026hellip; it requires constant monitoring, must detect failures, and automatically recover. Storage Needs : The system must be able to store millions of files, and multi-GB files are really common, but it should also support small files. WorkLoads Reads : Reads are mostly large sequential reads (major requirement), with a few random reads at a given offset. Writes : Mostly appends to a file (major requirement), with ****a few writes at a given offset. General Architecture Of GFS there are mainly two types of servers\nMaster Server : It is like a leader that handles and governs everything. All reads and writes must initially go through the master server. It stores file-to-chunk mapping and chunk-to-chunk-servers mapping. Chunk Server : These are worker server, which store the chunks. How Does GFS Store Data Say we have to store something really large, of size say 1 TB. We can only store it on a single computer if it has enough storage. And if it does not have enough storage, the next best thing to do is to chop up the file into several chunks and store those chunks on several machines.\nGFS mostly does the same; here a file is stored as several 64 MB chunks across several servers. The servers which store these chunks are called chunk servers. Also, these chunks are replicated across multiple servers for high availability and redundancy.\nEach chunk is identified by an immutable and globally unique 64-bit chunk handle.\nThis technique of splitting is very widely used in Virtual Memory in Operating Systems and TCP (fragmentation and reconstruction).\nNow we know that a single file is split into multiple chunks and stored across several servers. So how do reads, writes, and deletes work in GFS??\nRead Flow Reads are pretty simple in GFS.\nThe application asks the GFS client, \u0026ldquo;I want to read this file and this offset.\u0026rdquo; The GFS client translates the offset into a chunk number and asks the master for the chunk servers from where the client can read this chunk from. The master server returns a set of chunk servers corresponding to the chunk along with the chunk handle. The client then asks those chunk servers for the chunk. Finally\u0026hellip;!! The chunk will be transferred from the chunk server to the client. That’s for the happy path…\nHandling Failures Here, the major failure occurs when the chunk server is down. In that case, the client can just ask another chunk server for the chunk, as the master server would have shared the location of all the chunk servers where the chunk is stored.\nWrite Flow It is the writes that are complex\u0026hellip; There are different types of writes:\nWrites: Data to be written at a specific offset. Record Appends: Append record (data) to the file. For me, it was the write part of GFS that was quite a bit confusing\u0026hellip; There are many variables here: what if the write is greater than chunk size, what if the write fails, and so on.\nWrites at Offset Data Flow When we have writes, concurrent writes are always the problem\u0026hellip; The major solution for concurrent writes is a single source of truth or some sort of leader.\nEach chunk is replicated across multiple chunk servers, and to handle these writes across multiple chunk servers, the master uses a technique called lease management, where one of the chunk servers acts as primary and others act as secondary. To edit that chunk, the write command must pass through the primary replica.\nWhen a client wants to write at a particular offset in a file:\nThe client first asks the master for the chunk servers. The master gives the client the chunk handle and chunk server locations. Once the client knows about the primary and secondary chunk servers, The client starts sharing the data to be written with all the chunk servers. Here, as you see in the diagram, the file is first sent to the secondary server A, and the secondary server A then sends data to the primary server, and again, the primary server sends data to the next secondary server B. Here the data is shared in a pipeline fashion in order to maximize the network bandwidth. The client shares the data with the server that is closest to it (here, it is the secondary server A), and then the chunk server shares the data with another chunk server that is closest to it, and so on… It\u0026rsquo;s not like the secondary server A waits for the entire data to be transferred and then sends it to the next server. It would be very slow. As soon as it has some data, it starts pushing it to the next server. The chunk servers will not yet commit these writes but will just store them in an LRU Buffer Cache. Once the data transfer is complete, the client receives an acknowledgment for the data transfer from all the chunk servers. Then the client issues a write command to the primary chunk server. Here, there can be multiple concurrent writes happening, but the primary chunk server gives these writes an order. It is just like saying, \u0026ldquo;You go first, you go second, and you go third,\u0026rdquo; and so on… Then it applies the writes to its chunk in the given order, and it then tells other secondary servers to do the same. Secondary replicas acknowledge the writes to the primary replica. Finally, the primary replica sends an acknowledgment to the client, concluding the write successfully. Now all the chunk servers have the writes in the same order. And that\u0026rsquo;s the happy path. And many things can go wrong here, like the primary chunk server might fail, or the secondary chunk servers might fail, or the chunk might get corrupted, and so on.…\nHandling Failures Here the main solution for handling failures is retrying the operation again and again. Primary replica fails\u0026hellip; retry. Secondary replica fails\u0026hellip; retry.… Retries would not be a problem, as writes at a particular offset would not lead to duplication even on retries. That is because here the client is trying to edit the same region again and again. Failures would just leave the chunk in an undefined state; however, a successful retry would just overwrite it with defined data.\nFew Interesting Bits GFS separates control flow and data flow, where data flow is the transfer of data between the servers and control flow is like the issuing of commands, requests, or giving instructions.\nThis separation allows multiple clients to transfer files (data flow) to chunk servers without blocking one another, as the actual write takes place only when the primary chunk server issues a write command (control flow).\nRecord Append Data Flow In an append record flow, the client just gives GFS the data to be written, and GFS appends it and gives back the client the offset where it wrote.\nRecord append is similar to the write flow, but here, the chunk is just the last chunk of the file. GFS has a few extra steps in order to guarantee atomicity even when there are concurrent writers, such as the maximum size of the record being limited to 16 MB; this is to reduce fragmentation.\nIn the record append data flow, similar to the write operation, the client sends the data to the primary and the secondary chunk servers. When the client issues the append operation to the primary chunk server, the primary chunk server first checks if it fits in the current chunk. If it does, then the record is written, and its offset is returned to the client.\nIf the record does not fit in, then the primary chunk server pads the data, asks secondary servers to do the same, and asks the client to retry on the next chunk. Now the client retries by asking the master for the next chunk, which does not exist yet. So the master allocates a new chunk and replies to the client with the new chunk. Now the data is written in the new chunk.\nHandling Failures Here, just like with writes at offset and reads\u0026hellip; the way of handling failures is retries. But the catch here is that, unlike writes at an offset, record appends lead to duplicates, but given the workload of the system, it is generally fine, and the GFS client can also be configured to handle duplicates.\nGFS promises that record append writes to the chunks at least once.\nConcurrent Writes at Offset and Record Append Operations Concurrent writes at offset might leave the chunk in an undefined but consistent state, but that is not quite the case for record appends here.\nConsistent state ⇒ all the chunk servers see the same data. Defined state ⇒ a client sees what it writes Here is an example.\nLet\u0026rsquo;s assume there is a chunk with the following state: 1 2 3 4 5 6 7 8 9 10.\nClient A wants to write the data AAAAAAAAAA at an offset 0.\nClient B wants to write BBBBBB at offset 5.\nHere, even though these operations are serialized with the help of lease management, it still leads to an undefined state.\nLet\u0026rsquo;s assume Client A\u0026rsquo;s write operation goes first, then Client B\u0026rsquo;s write operation. This would leave the chunk in the following state: A A A A A B B B B B B\nSo, here the state is undefined because even though Client A\u0026rsquo;s write is successful, it does not see all the data it wrote. But this is not the case with record appends.\nOn concurrent record appends, it is not possible as GFS is not writing the data at an offset but tries to add the data to the end of the file. Again, the writes are ordered by the primary, so all concurrent writes are serialized. Here, let\u0026rsquo;s assume there is enough space in the chunk for both Client A\u0026rsquo;s and Client B\u0026rsquo;s data, and the primary first appends Client B\u0026rsquo;s record, then Client A\u0026rsquo;s record. This would leave the chunk in the following state: 1 2 3 4 5 6 7 8 9 10 B B B B B B A A A A A A A A A A\nHere, when the concurrent writes are successful, it leaves the system in a defined state.\nAlong with reads and writes, there are many parts of GFS that keep the system up and running…\nDeletion and Garbage Collection Deletion and Garbage Collection Deletion in GFS is a bit interesting. It just renames the file name to a hidden name. The name includes the timestamp of the deletion, and it is kept in the metadata for some time, like 3 days by default (configurable). During this duration, the file can be recovered.\nThe master server periodically scans the metadata, and during these periodic scans of the file system, if the hidden file is older than 3 days, it deletes it from the metadata (remember, chunks in the chunk servers are not yet deleted), making all the chunks of the file orphan, i.e., not reachable.\nWhen the heartbeat messages are exchanged and the master notices that the chunks are orphaned, the master tells the chunk servers to delete the chunk.\nOther Parts of GFS Heartbeats The master and the chunk servers communicate periodically through heartbeat messages. These messages are really important, as the states of the chunk servers and commands to the chunk servers are all piggybacked onto these heartbeat messages.\nHandling Stale Data So\u0026hellip; what if a chunk server goes down, a write happens, and after a long time the chunk server comes back up? Now the chunk server holds stale data. How is this handled?\nStale data is handled by using a version number. Each chunk is given a version number, which is incremented during mutations, i.e., writes or record appends.\nIt is through the heartbeats that a chunk server tells the master about the chunks that it has and the versions of the chunks.\nThe master notices that other chunk servers holding the same chunk handle have a newer version. It simply considers the chunk to not exist at all, making it an orphan chunk which will be removed during garbage collection, and it issues re-replication in order to maintain the replication factor of the chunk.\nThe Master…. Metadata The master stores the following metadata:\nFile and chunk namespaces The mapping from files to chunks Location of chunk replicas File and chunk namespaces and the mapping of files to chunks. This metadata is persisted by logging changes to the operation log.\nOperation Logs The operation log is similar to WAL (Write-Ahead Log) in databases: before making any changes, first write to disk, make the change, and reply to the client.\nThe operation log is also replicated to other servers for redundancy so that whenever the master goes down, when it comes back up, it replays the operation log to recover the state of the \u0026ldquo;file and chunk namespace\u0026rdquo; and \u0026ldquo;files to chunks mappings.”\no speed up recovery, it uses checkpointing. A checkpoint is like a snapshot of namespaces and file-to-chunk mappings. Whenever the size of the log is greater than a certain threshold, the master switches to a new log file and, along with it, creates a new checkpoint in another thread.\nThese structures are designed in such a way that a new checkpoint can be created without delaying incoming mutations.\nSo\u0026hellip; whenever it comes back up, it should only replay a few logs from the most recent checkpoint.\nAdditionally, the master also does a lot of things, such as rebalancing chunks by moving them to different chunk servers.\nRebalancing Chunks There are several things the master considers regarding chunk placements:\nAll chunks must be placed on different racks. It places chunks on chunk servers with below-average disk utilization. Place the chunks on chunk servers which do not have any recent writes/chunk placements. Back to Failures… Now, getting back to failures, what if a chunk gets corrupted??\nHere, to determine the integrity of the chunk, checksums are used. Every 64 KB has a 32-bit checksum.\nThere are mainly two ways in which the chunk server finds out it has corrupted chunks:\nDuring Regular Chunk Scanning ⇒ The chunk server regularly scans the chunks for their integrity. During Client Reads ⇒ When a client wants to read a chunk, the chunk server first checks the checksums before sending the chunk to the client. During these scenarios, if the chunk server finds out that the chunk is corrupted, it tells the master, and the master issues a replication from another chunk server.\nNow\u0026hellip; what if a chunk server fails??\nWhen a chunk server fails, it does not send heartbeat messages. The master notices the absence of heartbeats, which decreases the replication factor of the chunks belonging to that chunk server, and now to maintain the replication factor of the chunks (here, replication factor just means how many replicas of the chunk should be present), the master server creates a new replica of the chunk.\nOne More Final Interesting Thing Here, in order to maintain high availability, GFS uses something called a shadow server. GFS maintains a shadow master server. As the name implies, the shadow master server is a bit stale.\nThis shadow master server serves the Read Operation when the master is down and it also keeps itself up to date by reading the operation logs of the master server. This shadow master server greatly improves read availability when the master goes down.\nGFS looks really simple, but it\u0026rsquo;s quite complex with so many moving parts. Hope you enjoyed the read and now better understand GFS. Next up is the RAFT consensus algorithm.\n","permalink":"http://localhost:1313/posts/google-file-system/","summary":"Parts of GFS GFS (Google File System) is a reliable distributed file system that is built on top of unreliable commodity machines.\nCommodity machines break down, and they break down pretty often, and given that there are many such machines, at any given moment it is a miracle if all the machines are working fine.\nSo, it must constantly monitor itself and detect, tolerate, and recover from component failures regularly.","title":"Google File System"},{"content":"Map-Reduce Before getting into MapReduce, let’s first understand what distributed systems are — and why they’re widely used.\nImagine You have some work that can be done by a computer. What do you do? Easy — you just assign the work to your computer. Now the work gets harder, and it’s taking longer to complete. What do you do? Simple — buy a better CPU, GPU, more RAM. Upgrade your computer. That’s awesome. But then the work gets even harder. What now? You upgrade again — even better CPU, GPU, and RAM. Now you’re running the latest, top-notch machine. You’ve maxed out how strong your computer can be. But the work keeps growing. It takes more time again. And now, you can’t make your computer any stronger. Looks like we’ve hit a wall, right? Actually\u0026hellip; no. What if we make two computers do the work? Hmmmm. And if the work grows again? Just add two more computers. Then four. Then ten. This is called horizontal scaling — adding more machines instead of upgrading just one. And when multiple computers work together on a problem, we call that a distributed system.\nSo what is MapReduce, and how does it fit into distributed systems?\nThink of MapReduce as a ninja technique that helps us split up work across multiple computers and stitch the results back together — all while making the process faster and more efficient.\nLet’s break it down with an example.\nThe Problem: Sum of Numbers Across Multiple Computers\nImagine you have 10 computers, and each stores 100 numbers.\nComputer 1: numbers 1–100 Computer 2: numbers 101–200 … and so on. Now, you want to compute the sum of all 1,000 numbers.\nNaive Approach: You let one computer do the work. It asks the other 9 for their numbers over the network.\nIf it takes:\n1 second to add two numbers 0.1 second to fetch a number from another computer 0 seconds if the number is already local Here’s the catch:\nOnly 100 numbers are local. The remaining 900 need to be fetched.\nFirst 99 additions (local): 99 seconds Next 900 additions (remote): 900 × (1 + 0.1) = 990 seconds Total time: 99 + 990 = 1,089 seconds\nThat’s sloooow.\nMapReduce-Style Approach: Now, What if each computer adds up its own 100 numbers in parallel?\nComputer 1: sum = 5050 Computer 2: sum = 15050 Computer 3: sum = 25050 … and so on. Each computer performs 99 additions locally (no network delay).\nTime taken per computer: 99 seconds\nOnce done, they send their 10 partial results to Computer 1.\nSending 9 results: 9 × 0.1 = 0.9 seconds Final 9 additions: 9 seconds Total time: 99 + 0.9 + 9 = ~108.9 seconds\nThat’s 10 times faster!\nAs you can see, the MapReduce style has two key phases:\nThe phase where each computer adds up the numbers that are local to it — this is called the Map Phase. The phase where all computers send their local results to one computer for the final showdown — this is the Reduce Phase. Now let’s get into the technical details of how map reduce actually work…!!! master, preprocess and create tasks map tasks, reduce tasks, workers ask for tasks send results to master, fault tolerance,\nNow, let’s dive into some technical details of how MapReduce is actually implemented.\nHow does MapReduce work? Imagine you own a massive digital library with 100 terabytes of .txt files distributed across 10 computers.\nYour task is to count how many times a particular word appears across all the books.\nTo do this efficiently, you can use MapReduce. But before we get started, we need to define two functions: the Map function and the Reduce function.\nWhat is a Map Function?\nThe Map function takes a portion of the text and maps it into key-value pairs. These key-value pairs are also known as intermediate key-value pair\nNow, what exactly should the Map function do?\nIt will break down a chunk of text (from one of the files) and produce a key-value pair for each word in that text. The key will be the word itself. The value will always be 1 (because we’re just counting the occurrences of each word). So, if the word \u0026ldquo;dog\u0026rdquo; appears 2 times in a particular block of text, the Map function will output:\nKey: \u0026ldquo;dog\u0026rdquo;, Value: 1 (for each occurrence). What is a Reduce Function?\nIt takes all key-value pairs that share the same key and combines them into a single result.\nFor instance, if we have many key-value pairs for the word \u0026ldquo;dog\u0026rdquo;, like: \u0026ldquo;dog\u0026rdquo; → 1 \u0026ldquo;dog\u0026rdquo; → 1 \u0026ldquo;dog\u0026rdquo; → 1 \u0026ldquo;dog\u0026rdquo; → 1 The Reduce function will combine these values and output a single pair:\nKey: \u0026ldquo;dog\u0026rdquo;, Value: 4 (the total count of the word \u0026ldquo;dog\u0026rdquo; across all the chunks). How Does MapReduce Work in Practice? Now that we have our Map and Reduce functions, let’s look at how MapReduce actually runs in a distributed system to give us the results we want.\nFor simplicity, let’s assume the master and reducers are separate computers. But in reality, the workers themselves perform the reduce tasks, and the master is only responsible for task assignment and monitoring.\nThe Master and Worker Computers\nIn a MapReduce system, we have two types of computers:\nMaster Computer: The job of the master is to assign tasks to the worker computers. Worker Computers: These are the machines that actually process the data and perform the work, running the Map and Reduce functions. But how does the master know which task to assign to which worker? Here’s where things get clever.\nTask Assignment and File Distribution Each worker computer needs to access the data stored on the network, but if the required file is not on the computer, it would need to fetch it from another machine. Network transfer is slow, so we want to minimize this delay.\nThe Master Computer is smart. It knows exactly where the data is stored and assigns tasks so that, as much as possible, the worker computers can access the required data locally, without the need for network transfers.\nExample: File Distribution: The master knows which files are stored on which computers. For instance: Computer 1 stores files 1, 2, 3, 5, 7. Computer 2 stores files 6, 8, 9, 10, 11 and so on …. The Preprocessing Step Before assigning tasks, the master computer pre-processes the files and splits them into map tasks. The size of each map task is typically between 16MB to 64MB.\nhere the pre-processing does not include reading the entire file, it mostly only processes the meta data like the size of the file.\nThese map tasks are chunks of the files that can be processed independently.\nTask Assignment to Workers Let’s say that the master needs to assign a task to Computer 1. If Computer 1 has File 1, the master will assign Map Task 1, which corresponds to part of File 1.\nHere’s how the tasks might be split:\nFile 1 is split into Map Tasks 1, 2, 3, \u0026hellip; File 2 is split into Map Tasks 12, 13, 14, \u0026hellip; If Computer 1 requests a task, the master will likely assign it Map Tasks 1, 2, and 3, since File 1 is stored on Computer 1. Similarly, if Computer 2 requests a task, the master will assign it Map Tasks 12, 13, and 14, as those tasks correspond to File 2, which is stored on Computer 2.\nHow are Map Tasks Processed Map Phase The map and reduce functions are distributed to all worker computers by the master. So, every worker has a copy of these functions.\nWhen a worker receives a map task, it reads its assigned chunk of the input file and applies the map function to it. This produces a set of key-value pairs. These key-value pairs are also known as intermediate key-value pairs.\nBut instead of storing all the key-value pairs in one place, the worker partitions them into R intermediate buckets — where R is the number of reduce tasks.\nEach bucket contains a subset of the key-value pairs. The partitioning is usually done based on the key — often using a hash function, like:\nbucket_number = hash(key) % R This ensures that:\nAll values for the same key always go to the same bucket, and Each reduce task will receive one bucket from every map worker, containing all the key-value pairs it needs to process. Map to Reduce Transition In this example, we have 3 reducers, so the output from each map function is split into 3 separate reduce buckets — one for each reducer.\nEach worker processes multiple map tasks, and each task produces key-value pairs. These key-value pairs are partitioned into three buckets, indicated by red, green, and blue in this example.\nWhen a map task finishes, its intermediate results are stored in these buckets. The addresses (locations) of the buckets are then sent to the master — not the actual data.\nThink of it like passing around pointers or references to the data — not the data itself.\nReduce Phase Now it’s time for the reducers to step in.\nThe master now holds the addresses of all the intermediate results from the completed map tasks.\nDon\u0026rsquo;t get confused by the visuals — each bucket shown here just represents the location of the data, not the data itself.\nwe have three reducers and master with addresses of all the results we got from computing map tasks.\nEach reducer is assigned all the buckets of a specific color (or partition):\nReducer 1 gets all red buckets, Reducer 2 gets all green buckets, Reducer 3 gets all blue buckets. Let’s zoom into Reducer 1 to see what it does:\nReducer 1 now fetches all the actual data from the bucket addresses it received. Since this involves data transfer over the network, it can be a bit slow.\nOnce Reducer 1 collects all key-value pairs with the same key, it runs the reduce function on them and writes the final output to a file.\nThis same process happens in parallel for Reducers 2 and 3.\nOnce all reducers finish it’s DONE!\nSorting Example now let’s see how sorting works using map reduce. the sorting confused me a bit, because i was thinking of memory constrains during the reduce phase. for the sorting to be done you must have enough space in your reducer to stored the output. in sorting the data is not moved around but the copy of the data is moved. so if the size of the data to be sorted is N we must have extra N free space left to store the result.\nsorting algorithms like external merge sort and external M-way merge are used to sort and merge the data that does not fit in memory.\nsay you have 10 numbers range between 1-100 in 10 computer that must be sorted and you have 2 reducer. and partitioning of the intermediate keys is done using the following:\nif number \u0026lt;= 50 { // bucket-1 } else { // bucket-2 } here is how the state of the computers currently look like.\nnow the workers are assigned map tasks and the numbers are sorted locally, for simplicity let’s assume each worker is assigned a single map task and the map task contains includes all the numbers present in the worker.\nnow the numbers are sorted locally within the worker. here is current state of the workers.\nnow it’s time for the reduce phase, as discussed before the workers will send back the master computer the addresses of the intermediate buckets, these addresses are sent to the reducers.\nlet’s zoom into the working of the reducer 1, here the reducer fetches all the buckets, and performs m-way merge, if the data is too large, it streams the data from the worker so that it’s not overloaded. and if the data is too large to fit into memory it performs m-way external merge.\nand similarly reducer 2 performs the reduce task and finally we get two output files containing numbers in the sorted order, remember these are just copy of the original number that are in the computers.\nSome Interesting Stuff Now that we\u0026rsquo;ve gone through how MapReduce works conceptually, let’s get into a practical bit — my implementation of file splitting for MapReduce. This is part of my solution for Lab 1 of the MIT 6.5840 course.\nHere, I’ll talk about one interesting challenge: splitting a file into chunks that preserve full lines.\nThe Problem: Chunking Without Breaking Lines Let’s say we have a file with 5 lines.\nIf we naively split it into fixed-size chunks, it might look like this:\nAs you can see, the line \u0026quot;they play outside.\u0026quot; is split between two chunks. The character 'T' is in chunk 2, while the rest is in chunk 3.\nThis is problematic because I wanted each map task to be able to process the file line by line, which becomes difficult when lines are split across chunks.\nThe First (Bad) Idea 💡 At first, I thought: “Why not just read the file line by line and store those lines in tasks?”\nBut that turns out to be a really bad idea.\nWhy? Because it forces the coordinator (or master) to read the entire file — defeating the point of MapReduce, especially if you have thousands of files, each potentially gigabytes in size. This kills scalability.\nThe Right Approach Instead, we should create chunks based on metadata (like file size), without reading the file content during task creation.\n// creates list of MapTasks from chunks func createMapTasks(files []string, nReduce int) []MapTask { slogger.Info(\u0026#34;Creating map tasks\u0026#34;, \u0026#34;nReduce\u0026#34;, nReduce, \u0026#34;files\u0026#34;, files) tasks := []MapTask{} for _, file := range files { chunks := createChunks(file) for _, chunk := range chunks { task := MapTask{ Id: uuid.NewString(), Chunk: chunk, NReduce: nReduce, } tasks = append(tasks, task) slogger.Info(\u0026#34;Created map task\u0026#34;, \u0026#34;taskId\u0026#34;, task.Id, \u0026#34;file\u0026#34;, file) } } return tasks } // splits the file into chunks func createChunks(filename string) []Chunk { fi, err := os.Stat(filename) if err != nil { log.Fatal(\u0026#34;reading file stat:\u0026#34;, err) } size := fi.Size() count := math.Ceil(float64(size) / float64(CHUNK_SIZE)) tasks := []Chunk{} for i := range int(count) { task := Chunk{ Filename: filename, Offset: int64(CHUNK_SIZE * i), Size: int64(CHUNK_SIZE), } tasks = append(tasks, task) } return tasks } Here, I’m using just the file size to define chunk boundaries — no content is read at this point.\nHandling Line Boundaries When Reading Now comes the tricky part: reading the chunk while ensuring we don’t break lines.\nThe logic:\nIf the last character of a chunk isn’t \\n, it means the line continues into the next chunk. So, the next chunk will skip the first line (since it was already partially included in the previous one). Additionally, the current chunk might need to grab the remaining part of the line from the next chunk. Here’s how I implemented it:\n// readChunk reads a chunk and returns full lines, handling line splits at boundaries func readChunk(task Chunk) []string { ... // Check the last byte of the previous chunk if task.Offset != 0 { prevChunkLast := make([]byte, 1) ... if prevChunkLast[0] != \u0026#39;\\n\u0026#39; { skipStart = true } } ... // Read lines using bufio.Scanner scanner := bufio.NewScanner(bytes.NewBuffer(chunk)) for scanner.Scan() { if i == 0 \u0026amp;\u0026amp; skipStart { i++ continue } lines = append(lines, scanner.Text()) } // If this chunk ends in the middle of a line, grab the rest from the next chunk if checkNext { ... } return lines } This way, each map task processes only complete lines, and the coordinator doesn’t need to read entire files — staying true to the spirit of distributed processing.\n\u0026hellip;and then I realized — the test cases were actually applying the mapf function on the entire file directly 😅.\nSo technically, I didn’t even need to split the files at all. LOL.\nIf you\u0026rsquo;re curious, you can check out my full implementation (including this chunk-splitting logic and more) here:\n👉 https://github.com/sanjayJ369/dist-db\n","permalink":"http://localhost:1313/posts/map-reduce/","summary":"Map-Reduce Before getting into MapReduce, let’s first understand what distributed systems are — and why they’re widely used.\nImagine You have some work that can be done by a computer. What do you do? Easy — you just assign the work to your computer. Now the work gets harder, and it’s taking longer to complete. What do you do? Simple — buy a better CPU, GPU, more RAM. Upgrade your computer.","title":"Map Reduce"},{"content":"Schedule Tracker The main purpose of this project is to digitize the progress tracking I currently do with the help of my diary. Why did I choose this project? It’s fairly simple, and I haven’t built an end-to-end application before (other than the webhook tester). This project will include features like user accounts, logins, authentication, and more.\nHere’s a high-level overview of what I want to achieve:\nSessions Tracking I usually track my daily progress using sessions. While it’s not a perfect measure of the work done, I think it’s a fairly good indicator.\nI break tasks into 45-minute chunks, calling each chunk a session (you could adjust this to 1 hour, 1.5 hours, etc.). For smaller tasks, I use 30-minute chunks and call them mini-sessions 😅.\nHere’s what I want to track:\nNumber of sessions Gaps between sessions Currently, I track my sessions and daily session counts manually. With the data collected, I want to generate graphs and analytics.\nMiscellaneous Goals In addition to tracking sessions, I also want to track specific goals like meditation, gym workouts, etc. Users will be able to define multiple goals and keep track of progress for each one.\nI plan to create a system similar to GitHub’s contribution graph, where reaching more goals results in more green squares.\nNext Steps This was a high-level overview of the project. The next step is to create an event modeling diagram to better understand the systems involved, the tasks required, and the goals to achieve.\n","permalink":"http://localhost:1313/posts/schedule-tracker/","summary":"Schedule Tracker The main purpose of this project is to digitize the progress tracking I currently do with the help of my diary. Why did I choose this project? It’s fairly simple, and I haven’t built an end-to-end application before (other than the webhook tester). This project will include features like user accounts, logins, authentication, and more.\nHere’s a high-level overview of what I want to achieve:\nSessions Tracking I usually track my daily progress using sessions.","title":"Schedule Tracker"},{"content":"Sorting and Aggregates Sorting What is sorting?\nSorting is just arranging elements in a specific order. But why is this important for database management systems (DBMS)?\nWhy does sorting matter in DBMS?\nDBMS needs sorting for two main reasons:\nSQL uses the ORDER BY clause to return data in a specific order. Sorting helps make query processing more efficient. When records are sorted, operations like joins and ORDER BY can be done faster. Types of sorting:\nThere are two ways data can be sorted: physically and virtually.\nPhysical sorting means the data is actually stored on the disk in sorted order.\nVirtual sorting refers to the logical arrangement, like in a sorted B-Tree. Here, the records are arranged in order for quick access, but the physical storage on disk might not be sorted the same way. Why does physical sorting matter?\nImagine you sorted records based on a certain key and now need to access them one by one. If the physical sort order is different from the logical order, it will cause random disk I/O operations, which are slow. Disks perform much better with sequential access rather than random access.\nIn a clustered B-Tree, the sort key used in the B-Tree is the same as the physical sort order on disk, leading to faster access.\nIn an unclustered B-Tree, the sort key of the B-Tree differs from the physical order on disk, causing slower random access.\nWe know that the data in a database can be much larger than the available memory. Sorting data that doesn\u0026rsquo;t fit in memory is called external sort.\nSome common sorting algorithms include:\nTop N Heap Sort: This algorithm is used when we want to get the top N rows from the results. It works by creating a heap of size N and going through the rows to update the heap. Time complexity: O(n log n) Space complexity: O(1) Here is a great video if you want to learn more about heap sort: https://youtu.be/HqPJF2L5h9U Quick Sort: Quick sort organizes elements by dividing them around a chosen pivot. Time complexity: O(n log n) Space complexity: O(n)(The time and space complexity of quick sort depend on how the pivot is chosen.) Insertion Sort: Insertion sort is a simple algorithm where elements are added one by one into a sorted array. It’s similar to sorting playing cards. Time complexity: O(n²) Space complexity: O(1) Merge Sort: In merge sort, the array is divided into two halves, and each half is divided again until we have single elements. Then, these parts are merged back together to create a sorted array. Time complexity: O(n log n) Space complexity: O(n) The choice of sorting algorithm also depends on the initial arrangement of the elements. For example, if the elements are mostly in reverse order, simpler sorting algorithms like Insertion Sort and Bubble Sort perform much better. However, this is not always the case.\nHow to Sort Large Data Sets When the data to be sorted is too large to fit in memory, we use a technique called external merge sort.\nExternal Merge Sort External merge sort has two main phases:\nCreation of Runs:\nIn this phase, \u0026ldquo;runs\u0026rdquo; refer to chunks of sorted pages. A one-page run means the size of the run is one page, and the elements in that run are sorted. A two-page run means the size is two pages, and the elements in those two pages are also sorted, and so on.\nMerge of Runs:\nIn this phase, we merge two or more runs together to create a new run where all the elements are sorted. The size of the new run is equal to the total size of the previous runs that were merged.\nExample of External Merge Sort Let’s say we have 8 pages of data to sort, but our memory can only hold 3 pages at a time.\nCreation of Runs:\nFirst, we load all the pages into memory and sort them using an in-memory sorting algorithm based on our needs. After sorting, we store these pages back on the disk, which we will call one-page runs.\nMerging Runs:\nNext, in the merge phase, we load 2 pages, one from each run (here each run has only one page). With only 3 pages of memory available, we use one page to store the output. We then merge the two pages into the third page. When the output page is full, we write it back to disk. After finishing with the page from the run, we load the next page from the run, similar to the merge step in the merge sort algorithm.\nContinuing the Process:\nNow that we have two-page runs, we repeat the process by loading one page from each of the two-page runs into memory and merging them into a single page. We then write this back to the disk. When we finish with a page, we load the next page from the run and continue this process.\nThis process continues until all the data is sorted.\nCost Analysis of External Merge Sort The external merge sort algorithm has two main components: the number of passes and the number of I/O operations. For each pass, we perform 2*N I/O operations: N read operations to load the pages into memory from disk and N write operations to save the sorted pages back to disk. While we cannot reduce the number of I/O operations, we can optimize the number of passes.\nIn the algorithm we discussed earlier, during each pass, we merge 2-page runs to form 4-page runs, and so on. This means the total number of runs is halved with each pass. It\u0026rsquo;s important to note that a 4-page run means there are 4 pages in that run, not 4 separate runs. This was a bit confusing for me initially.\nThe total number of passes can be expressed as $\\log_2(N) + 1$ , where the \u0026ldquo;+1\u0026rdquo; accounts for the initial pass where we sort the individual pages. In this case, we have only 3 pages in the buffer. However, if we have B pages in the buffer, we can merge B-1 runs at a time (k-way merge). During the initial pass, we can load B pages into memory and sort them directly, producing B-page runs. Therefore, we start merging with $\\lceil \\frac{N}{B} \\rceil$ , leading to the new equation:\n$$ log_{B-1}(\\lceil \\frac{N}{B} \\rceil) + 1 $$\nWe know that disks perform better with sequential access rather than random access. Previously, we only brought one page from a run into memory. This caused the disk head to move around a lot to retrieve a page from each run. To minimize this, we can load multiple pages from the same run at once, reducing random accesses. However, this comes with a trade-off: it limits the number of runs that can be merged at a time. For example, if we have 9 pages and bring in a single page from each run, we can merge 8 runs at once. But if we load 2 pages from a single run, we can only merge 4 runs at a time.\nIf we load chunks of pages from a run, say C pages, the equation becomes:\n$$ log_{\\frac{B}{C}-1}(\\lceil \\frac{N}{B} \\rceil) + 1 $$\nHaving a larger buffer significantly speeds up the sorting process. Let\u0026rsquo;s calculate the number of passes for ( N_1 ) (number of pages) = 1,000,000 and ( B_1 ) (buffer size) = 3, and for ( N_2 ) = 1,000,000 and ( B_2 ) = 50.\nWe can clearly see that increasing the buffer from 3 to 50 reduces the number of passes from 20 to just 4.\nOptimizations Double Buffer Optimization\nFrom the graph below, we can see that the output remains consistent over a large range of pages. Changing the buffer size only slightly affects the number of passes. While merging, the disk can remain idle. However, we can allocate some buffer pages for pre-fetching from the disk. This way, while merging is happening, the disk can load the next batch of pages into memory, speeding up the process.\nComparison Optimizations\nSome comparisons can also be optimized using specialized code. For example, if the data type is a string, instead of comparing the entire length of two strings, we can first compare just the first n characters. This can reduce the number of operations required.\nAggregates In aggregates, the result is produced by performing calculations over several elements, such as count, max, min, etc.\nIf an ORDER BY clause is used in the query, we need to sort the data. To do this, we can use the external merge sort method to order the elements.\nIf there is no ORDER BY clause, we can use hashing, which is generally faster than sorting.\nLike sorting, hashing has two phases:\nPartitioning:\nIn this phase, we partition the data into several chunks. The number of chunks is equal to B-1, where B is the number of buffer pages available. When we bring a page from the disk, we hash it to one of the pages using a hash function, say ( h_1 ). If the page overflows, we spill it to disk, creating several partitions.\nReparation:\nIn this phase, we bring in pages from each partition and hash them using another hash function, say ( h_2 ). Depending on the required aggregate operation, we might store a running sum of the necessary values. For example, if we want to calculate the average grade grouped by the course, we would store the running sum of the number of students in the course and the total sum of the students\u0026rsquo; grades. This information can then be used to calculate the final result.\nThe example below shows how we might store the course ID (key) and the total number of students in that course.\n","permalink":"http://localhost:1313/posts/sorting-and-aggregates/","summary":"Sorting and Aggregates Sorting What is sorting?\nSorting is just arranging elements in a specific order. But why is this important for database management systems (DBMS)?\nWhy does sorting matter in DBMS?\nDBMS needs sorting for two main reasons:\nSQL uses the ORDER BY clause to return data in a specific order. Sorting helps make query processing more efficient. When records are sorted, operations like joins and ORDER BY can be done faster.","title":"Sorting and Aggregates"},{"content":"This blog summaries the lecture 8 of cmu - Intro to Database Systems along with some things i learned on the way.\nConcurrency and Parallelism Parallelism:\nParallelism means running multiple tasks simultaneously. It can be achieved with the help of multiple cores, multiple threads, GPUs, etc.\nExample: In programming, running two different functions at the same time.\nConcurrency:\nConcurrency refers to the ability to run tasks out of order without changing the final outcome. Tasks can progress independently, and generally, these tasks are interleaved. This means the CPU might execute a part of one task, then stop and execute a part of another task, and so on.\nExample: In programming, if a program has two functions, say f1 and f2, where running these two functions in any order does not change the outcome, the result is the same whether f1 runs before f2 or f2 runs before f1. In this case, we can say that the program supports concurrency and these functions are concurrent.\nConcurrency in Index Why do we need the index to be concurrent? We need the index to be concurrent to support the execution of multiple transactions at once and to allow for maximum system throughput. Concurrency enables tasks such as reading, writing, updating, and deleting to be executed out of order. By allowing multiple transactions to be executed out of order, we can run these transactions in parallel while ensuring that the final output remains correct.\nHowever, when multiple transactions modify the index simultaneously, we must handle cases where one transaction might be reading a tuple while another transaction is changing it. In such scenarios, the output expected by the first transaction could be corrupted. To handle these cases, we introduce concurrency control mechanisms.\nWhat do we mean by correct output? There are two main criteria:\nLogical correctness: Logical correctness means that when a thread writes a tuple and then reads the same tuple again, it should see the tuple it wrote initially, provided that no other thread has updated it. This ensures that the data seen by a transaction is consistent with its own operations and the operations of other threads as managed by the concurrency control protocol.\nPhysical correctness: Physical correctness means that the data structure remains sound. This means it is not corrupted, does not hold pointers to invalid memory locations, and does not skip nodes or contain other structural errors. This ensures that the underlying data structure of the index is intact and operational.\nLocks and Latches As we discussed above, if one transaction is reading a node in an index and another transaction comes along and changes the node, the result expected by the first transaction can be corrupted. This means that the first transaction might read part of the node before the other transaction has made changes and the remaining part after the changes have been made. As a result, the output received by the initial transaction will be inconsistent. To handle these issues, we mainly use techniques such as locks, latches, and atomic transactions.\nEven though locks and latches sound similar, they are a bit different:\nLocks: Locks protect the contents of the database, such as tuples and relations, from being modified by other transactions while a transaction is performing operations on them. Locks are held for the entire duration of the transaction and support rollbacks if something goes wrong.\nExample: If a transaction is updating a tuple in a table, a lock is acquired on that tuple to prevent it from being modified by other transactions at the same time.\nLatches: Latches are used to protect the low-level data structures of the DBMS, such as indexes, page tables, etc. Latches are typically held for a short duration and do not need to provide rollbacks.\nExample: Using a latch to protect a node in an index from being modified by other transactions.\nDifferent ways of Latch implementation Blocking OS Mutex\nWe can use the mutex (mutual exclusion) functionality provided by the OS to protect shared data from being modified simultaneously.\nIn mutexes, we acquire a latch on the part of the shared data we are trying to modify. When another thread tries to modify the same part of the shared data, it is not allowed and is put into a waiting queue. Other threads will only be able to access the data when the initial thread holding the latch releases it. Here, the latching and unlatching are handled by the OS, which can be slow.\nA faster alternative in Linux is the futex, which reduces the number of system calls. For DBMSs, the DBMS itself can often do a better job of handling latches for internal data structures.\nTest and Set Spin Latch\nBefore the introduction of the test and set spin latch, the testing and setting of the latch used to be done with two different instructions. Think of a latch as a simple boolean value holding either false (0) or true (1).\nTesting: A thread checks the state of the latch. True means it’s latched, and false means it’s unlatched. Sometimes, it can happen that a thread checks the value of the latch and sees that it’s unlatched. Before it can acquire the latch (set it to true), the thread might be preempted (stopped) for some reason. During this time, another thread might check the latch value, see that it’s unlatched, acquire the latch, and continue its execution. When the CPU resumes the initial thread, it thinks the latch is still unlatched and acquires it. Now, we have two threads doing changes simultaneously, leading to unexpected errors.\nTo resolve this, the test and set instruction is used.\nIn the test and set instruction, testing and setting are combined into one atomic instruction. This ensures that either the thread gets the latch or it doesn’t, with no intermediate state where the latch can be acquired by multiple threads.\nSpin Latch: A spin latch means that the thread will repeatedly execute the test_and_set instruction until it acquires the latch.\nReader-Writer Latches\nThe problem with mutexes and test-and-set latches is that they don’t differentiate between the types of work being done by the threads. For example, if we have 10 threads that want to read some value, using a mutex or test-and-set latch would require each thread to wait for its turn to acquire the lock, which is inefficient.\nReader-writer latches solve this by introducing two types of latches: reader latches and writer latches. When threads are only reading the contents without modifying them, multiple threads can acquire multiple reader latches on the same critical section simultaneously. However, if a thread needs to modify the content, it uses a writer latch. A writer latch is an exclusive latch that can only be acquired when there are no reader or writer latches currently held.\nImplementation of Latches in Hash tables In hash tables, only a single slot is accessed by a thread at a time. By providing latches for the slots, we can support concurrency. This is true, but the implementation can vary based on the size of the critical section being protected.\nThere are two different types of latches based on the size of the critical section they protect:\nPage Latches: Here, latching and unlatching are done at the page level. If a thread wants to access a single slot, it acquires a latch on the entire page. This can put other threads into the waiting queue if they want to access slots in the same page. However, it simplifies the process for the thread holding the latch to read multiple slots within the same page. Slot Latches: Here, latching and unlatching are done at the slot level. If a thread wants to access a single slot, it acquires a latch only on that specific slot. This allows other threads to access different slots within the same page simultaneously. However, this approach requires maintaining a latch for every single slot in the hash table, which can be very expensive. Implementation of Latches in B+Trees implementation of latches in B+Trees is far more complex then the implementation of Latches in the hash table as in B+Trees a single thread might need to access multiple nodes and might also modify multiple nodes.\nUsing Simple Reader-Writer Latches\nWhen using reader-writer latches for indexing searches, we start by acquiring a latch on the root node of the B+Tree. As we proceed, we keep acquiring latches on the child nodes until we reach the leaf node, and finally, we acquire a latch on it. All the latches are held until the completion of the operation.\nIt’s not much of a problem if multiple threads want to search for a key-value pair and only acquire read latches. However, issues arise when we need to make changes. To modify the B+Tree, a thread needs to acquire a write latch. A write latch is exclusive, meaning only one write latch can be held at a time. If a thread wants to insert a new key-value pair into a B+Tree index, it needs to acquire a latch on the root node and maintain it until the leaf node where the insertion will take place. During this time, no other reads or writes can be performed because the root node is latched.\nThis causes a bottleneck and slows down the application. We solve this issue by implementing a few additional rules known as the crabbing latch protocol.\nUsing Crabbing Latch Protocol\nNow, let\u0026rsquo;s see what a crab has to do with latches!\nIn the Crabbing Latch Protocol, we prevent the bottleneck problem by following a pattern of acquiring and releasing latches. For example, if we want to search for a key-value pair, we first acquire a read latch on the root node, then a new read latch on its child node. Once the latch on the child node is acquired, we release the latch on the root node. This pattern of acquiring and releasing latches continues as we traverse down the tree.\nAcquiring write latches is a bit more complex because writing might lead to node splits, merges, or changes in the tree length. Before delving into each case, let\u0026rsquo;s define what a safe node is.\nSafe Node: A safe node is a node that can neither be split nor merged with other nodes if a new value is inserted or deleted from that node.\nSimilar to acquiring read latches, we first acquire a write latch on the root node and then on its child node. We then check if the child node is a safe node. If it is, it guarantees that even if a merge or split happens below the child node, it does not affect the root node(parent node). If the child node is safe, we release the latch on the root node(parent node). If it is not, we keep the latch and continue our traversal down the tree.\nHowever, for each write operation, we need to acquire a write latch on the root node, which can also lead to a bottleneck. Although it\u0026rsquo;s not as severe as before, it still slows down the system. To solve this, we make a slight modification.\nImproved Crabbing Latch Protocol\nThe reading process remains the same as before, but the handling of write latches is slightly modified.\nInitially, we use read latches for write operations as well. We traverse down the tree, acquiring read latches along the way. When we reach the leaf node, we check if we need to split or merge. If there is no need to split or merge, we simply perform the insertion or deletion.\nHowever, if a split or merge is needed, we restart the process, but this time by acquiring the write latches, just as before.\nhandling Leaf Node Scans\nLeaf node scans allow for traversal in different directions: left to right, top to bottom, and right to left. This flexibility can lead to potential deadlocks. For example, if thread A is waiting for thread B to release a latch and vice versa, a deadlock occurs.\nTo handle this situation, the best approach would be for one of the threads to detect the deadlock, wait for a specified amount of time, and then abort itself. This way, the deadlock is resolved, and the aborted thread (e.g., thread A) can restart its operation.\n","permalink":"http://localhost:1313/posts/index-concurrency/","summary":"This blog summaries the lecture 8 of cmu - Intro to Database Systems along with some things i learned on the way.\nConcurrency and Parallelism Parallelism:\nParallelism means running multiple tasks simultaneously. It can be achieved with the help of multiple cores, multiple threads, GPUs, etc.\nExample: In programming, running two different functions at the same time.\nConcurrency:\nConcurrency refers to the ability to run tasks out of order without changing the final outcome.","title":"Index Concurrency"},{"content":"Indexes in DBMS Indexes are very similar to the index section of a book. In books, indexes tell us where a word has been used and provide the page numbers. Just like that, indexes in a Database Management System (DBMS) tell us on what page or block a particular record is present.\nSo why do we need indexes?\nIndexes save us a lot of time. Instead of searching from the start to the end of the book for a specific word, we can simply look up the index and go to the specific pages. Likewise, in DBMS, if we want to retrieve a specific record, instead of searching every record in the database, we can just go to the specific page or block and retrieve the record.\nTypes of Indices Ordered indices: In ordered indices, the index is based on the sorted order of values. Hash indices: In hash indices, the values in the index are distributed over a range of buckets, and the value is hashed to get its index entry and then retrieve the location where the value is stored. Ordered Indices Clustering index: A clustering index is an index where the order in which the values are sorted in the index is the same as the order in which the values are stored in physical memory. Clustering indices reduce random access and are good for sequential access of records. They are also known as primary indices.\nNon-Clustering index: A non-clustering index is an index where the sort order on which the index is based is different from the order in which the values are stored in memory. If we want to access values over a range using non-clustering indices, there will be more random access. They are also known as secondary indices.\nIndex sequential files: files where the records are stored in a sorted order, and they also have an index based on the value on which they are sorted.\nindex entry: The search key value together with a pointer to the actual record in memory is known as an index entry.\nDense Index A dense index is an index where there is an index entry for every record present. In case of duplicate search keys, only a pointer to the first record having that search key is stored, and the rest of the records can be found by a simple sequential access as the index is a clustered index.\nDense indices can also be used on non-clustered indices, but the duplicate keys should be handled differently.\nLookups are performed by just traversing the index table, and if it\u0026rsquo;s based on a clustered index, we can use some optimization such as binary search.\nDense indices\nSparse Index A sparse index is an index where an index entry is present only for a few records. The size of a sparse index is smaller compared to a dense index. Sparse indices always need a clustering index as the lookup of the records which are not in the sparse index is not possible if it\u0026rsquo;s not a clustering index.\nLookups are performed by finding an index entry whose search key value is the greatest search key value present in the sparse index which is less than or equal to the search key value we need, and then doing a sequential search from that record.\nExample: If the search key value is 55, we aim to find an index entry in the sparse index that\u0026rsquo;s less than or equal to 55. If the sparse index contains entries like 45, 23, 25, and 35, we select 45. From there, we navigate to the record containing 45 and start a sequential search.\nSparse indices\nMultilevel indices When an index grows too large to fit in memory, searching for an index entry can become time-consuming due to increased disk I/O operations. To solve this issue, we use multilevel indices.\nIn a multilevel index, the large index table is divided into smaller sub-indices, also known as inner indices. A new index table, often called the outer index or root index, is created, where each index entry points to a specific sub-index of the original large index table. This allows for faster lookup of the relevant sub-index, enabling quicker search for the desired index entry.\nInsertion Dense Index In a dense index, if a new record is added to the database, it is appropriately placed in physical memory, and then a new index entry is made into the dense index, which points to the record.\nIf the dense index stores pointers to all the records having the same search key, under one search key, then a new pointer to the record is just added at the end.\nGiven that the index is a clustering index, If the dense index is storing only a pointer to the first record having the same search key value, then it\u0026rsquo;s not modified. If there is no index entry having the search key of the record being inserted, we create a new index entry.\nSparse Index If we assume an search key entry in a sparse index points to a block of records, and the index is a clustering index, then if a record is being inserted, it is checked if the new record holds the smallest value of the search key in that block. If it is, then we replace the old index entry with the new index entry, which points to the physical location of the newly inserted record. If not, it is not modified.\nDeletion Whenever a record is deleted from a relation, it should be removed from all the indices associated with that relation, so that we can avoid inconsistencies in query results.\nDense Index If a record is being deleted, first the search key value associated with that record is found, and then removed from the index.\nIf the dense index is storing all the pointers to the records having the same search key value, then the pointer to the record being deleted is removed.\nIf the dense index is storing only a pointer to the first record having the same search key value, then it is checked if the record being deleted is the first record. If it is, then the index entry is made to point to the next record having the same search key value.\nSparse Index It is checked if there is an entry of the record in the sparse index. If so, then the index entry is made to point to the next smallest value in that block. If not, the sparse index is not modified.\nSecondary Indices (Non-Clustered Indices) Secondary indices are also known as non-clustered indices. Here, the search key on which the index is constructed upon is different from the search key that the records are sorted and stored in memory.\nHere, the construction of a sparse index is not possible, but the construction of a dense index is possible. Even though the construction of a dense index is possible, the physical location of the values pointed by search keys might be stored in a different order.\nTo increase the access time, the search keys are first found, and then sorted based on their physical location, and then accessed.\nNon-Clustering index\nNon unique search keys If the search keys are non-unique, that is, whenever they are not based on primary keys, then the DBMS will generally concatenate the primary key to create a composite key of the non-unique search key to make it unique and then store it.\nNon-Unique Indices: These are the indices that allow the storage of non-unique search keys.\nB+ Trees The performance of indexes discussed earlier will degrade as the database size increases. However, B+ Tree data structures provide a consistent $O(log(n))$ time complexity for both insertions and deletions. The \u0026ldquo;B\u0026rdquo; in B+ Tree is not formally defined but is generally referred to as \u0026ldquo;balanced.”\nB+ Tree is like a evolution of M-way search tree\nM-way Search Tree An M-way search tree is an evolution of a binary search tree. In a binary search tree, each parent node has two child nodes: the left child and the right child. The value stored in the left child is less than the value of its parent node, and the value stored in the right child is greater than the value of its parent node.\nBinary Search Tree\nIn m-way search trees, the structure of the node is a bit different. Here, each node can store multiple values. An m-way search tree of order n has n pointers to child nodes and has n - 1 values.\nLet\u0026rsquo;s say a node of order n has pointers, p1, p2, ….., pn. and n - 1 values, say v1, v2, v3, … vn-1. The pointer p1 is present at the left end and pointer pn is present at the right end.\nThe values stored in the child node pointed by pointer p1 will be less than the value v1, and the values stored in the child node pointed by the pointer pn will be greater than the value vn-1. Let\u0026rsquo;s say pointer pa is present between values va-1 and va. Here, the child node pointed by the pointer pa stores the values that are less than value va and greater than the value va-1.\nM way node structure\nHere is an example of an M-way search tree of order 4:\nM-way search tree example\nB+ Tree Structure A B+ Tree is a special type of M-way tree with some additional rules:\nevery leaf node is of equal depth from the root node every inner node apart from root node is at least half full, that is number of pointers in a node is always greater then or equal to $⌈(n − 1)∕2⌉$ every inner node with k keys has k+1 non null children In a B+ Tree, the data that is a pointer to a record or the actual contents of the record are only stored in the leaf nodes. Here is the general structure of a B+ Tree:\nB-tree structure\nThe structure of non-leaf nodes is different from leaf nodes. Pointers in non-leaf nodes store a pointer to another non-leaf node or a leaf node.\nIn a leaf node, the value stored may be a pointer to the record containing the search key value or the actual contents of the record.\nHere is the general structure of a leaf node. The last pointer of the node points to the next leaf node.\nLeaf Node\nInsertion Inserting an index is straightforward if there is enough space in the leaf nodes. If we assume there is enough space in the leaf node, we simply find the node where we should add the new entry and add it.\nInsertion becomes more complex when there is not enough space in the leaf nodes, and the leaf node must be split. Here is an example of insertion:\nIf we want to insert an index entry and there is not enough space in the leaf node, the leaf node is split into two nodes. The first ⌈n/2⌉ values stay in the left node, and the rest of the values are moved to the new node. Now, the parent node is updated to include a pointer to the new node.\ninsertion-1\nIf there is not enough space in the parent node to accommodate the new left node created, we split the parent node. We first assume that there is enough space in the parent node by conceptually extending it. Then, we split the node, and the search key value that is present between the pointers that are kept in the left node and the pointers that are moved to the right node is moved up to its parent node.\nIn the example below, it\u0026rsquo;s 46. Here, there is no parent node, so we create a new parent node and add the search key value 46 to it. The new node becomes the root node and also increases the depth of the tree.\ninsertion-2\nDeletion Deletion in B+ trees is a more complex process than insertion. If deleting an index entry does not violate the rule of the node being at least half full, no changes are needed, and we can simply remove the index entry from the leaf node.\nHowever, if the leaf node has fewer pointers than the minimum required, i.e., $n\u0026rsquo; \u0026lt; ⌈(n − 1)∕2⌉$, where n\u0026rsquo; is the number of pointers present in the current node and n is the order of the B+ tree, then the leaf nodes must be merged or pointers in the leaf node must be redistributed.\nIn the example below, we first delete the record 45. Now, the leaf node is left with only one pointer, which points to the search key value containing 40. Since there is enough space in the sibling node, the pointer in the right leaf node can be merged with the left leaf node, and the empty node is deleted, along with the search key in the parent node.\nDeletion-1\nhere is the final state of the B+ tree\nDeletion-2\nNow, let\u0026rsquo;s see an example where we need to merge parent nodes. Assume the below B+ tree, and delete the search key 60.\nDeletion-3\nHere, the search key 60 is first deleted, then the leaf node containing it becomes underfull (it is not half-full). Since the sibling node has enough space to accommodate the remaining pointers, the search key 50 is merged with its sibling, and the search key 60 is removed from its parent node, and the empty node is deleted.\nNow, the parent node is underfull, and we cannot merge the pointers with the sibling node as the sibling node of the parent node is full (30, 40, 43). Here, we redistribute the pointers, moving the rightmost pointer of the sibling node to the right node.\nDeletion-4\nAs the pointers are redistributed, the pointer that separates these two pointers is not present in both nodes, but the search key value is present in the parent node, currently separating them, which is 46.\nHere, the parent node, i.e., the root node, should also be updated to have the correct search key value. The search key value from the sibling node is moved up.\nDeletion-5\nHere is the final state of the B+ tree structure:\nDeletion-6\nSometimes, deletion can reduce the size of the B+ tree. Now, assume the below B-tree:\nDeletion-7\nNow, if we delete the search key entry 47,\ndeletion of 47 from the leaf node makes it underfull. Now, the leaf node cannot be merged with the sibling node, so we redistribute the search keys. The rightmost search key 45 is moved left.\nDeletion-8\nHere, the 46 in the parent node no longer separates the two child nodes, so we correct it by changing it to 45.\nDeletion-9\nNow, we delete 46.\nIf we delete 46, the index entry 45 can now be merged with its sibling node, and the empty right node is deleted.\nDeletion-10\nNow, the parent node is underflowing, and it can now also be merged with its sibling. The search key value separating them is the value present in their parent\u0026rsquo;s node, which is 43.\nDeletion-11\nNow, the root node has only one pointer in it, so the root node is now deleted, and the depth of the entire B+ tree is now reduced.\nHere is the final state of the B+ tree:\nDeletion-12\nDesign Choices Node Size Generally, large node sizes are preferred over smaller ones, as the larger node size reduces the number of hard-disk access times, which can speed up the process. Typically, the size of each node is the same as the size of a single hard-disk page or block.\nMerge Threshold Sometimes, merging of nodes is not carried out in OLTP databases because the number of insertions and deletions is so high that the overhead caused by merging nodes can be significant. The CPU will spend more time merging nodes than actually performing insertions and deletions. This is also known as thrashing.\nVariable Length Keys Sometimes, the length of the search key can vary, for example, if the search key is a name of different lengths.\nPointer Here, a pointer to the search key value is stored instead of storing the search key value directly.\nVariable Length Nodes The search keys are stored normally, but the length of the node can change. This method is not generally used due to the large overhead in managing it.\nPadding The size of the search key is fixed, but for the search keys whose length is less, padding is added to make it fit in. This method is also not generally used due to the memory wastage in padding.\nKey Map/Indirection This is the most commonly used method. The structure of the node is similar to the slotted page, where we have metadata storing some information about the node. We then have an array of pointers pointing to the search key values in the node.\nIntra-Node Search The size of the node in B+trees is generally large, so we also need to use optimised search algorithms to find the required search key in the node.\nHere are some techniques used to search for the required search key:\nLinear Search Every search key in the node is traversed to check if it is the one that is required. The time complexity is $O(n)$.\nBinary Search The binary search technique is used to find the required search key.\n$O(log (n))$\nOptimization Prefix Compression Most search key values stored in a single node are likely to have a part in common. For example: 10001, 10006, 10009. Here, the search keys have 1000 in common. Therefore, only 1000 can be stored, and then 1, 6, 9 can be stored separately, hence saving some space.\nDeduplication Sometimes, the indexes support the insertion of duplicate values. Generally, a primary key is attached at the end to make it a unique composite key. Here, the duplicate search key can only be stored once, and the records can in turn be differentiated by only the primary key, which saves the space lost in storing the duplicate key multiple times.\nBulk Insert Sometimes, if we want to create a new B+tree for a relation, then if we insert the search key of each record one by one, it would lead to a lot of splits and would be slow. Instead, we first get all the search keys and their values, sort them, then turn them into leaf nodes and build the B+ tree from the bottom up.\nhere is the B+tree insertion and deletion example pdf\n","permalink":"http://localhost:1313/posts/tree-indices/","summary":"Indexes in DBMS Indexes are very similar to the index section of a book. In books, indexes tell us where a word has been used and provide the page numbers. Just like that, indexes in a Database Management System (DBMS) tell us on what page or block a particular record is present.\nSo why do we need indexes?\nIndexes save us a lot of time. Instead of searching from the start to the end of the book for a specific word, we can simply look up the index and go to the specific pages.","title":"Tree Indices"},{"content":"This blog is a continuation of the database systems series, covering Lectures 5 and 6 of the Database Systems CMU 15-445/645 course.\nBuffer Pool The buffer pool is the memory present in the main memory (RAM) that is used to store pages that are most frequently used, similar to a cache. It is managed by the DBMS (buffer pool manager), because the DBMS can do a better job of managing pages than the OS. If a query wants to read some records that are present on the disk, the corresponding page should first be fetched into main memory and then read.\nIn the buffer pool, pages are stored in an array of pages where each slot is called a frame.\nBuffer pools are managed by a Buffer Pool Manager.\nBuffer Pool Manager The Buffer Pool Manager is a sub-system of the DBMS that manages the storage present in the main memory (buffer pool). Accessing pages that are present in main memory is much faster than accessing pages that are present on the disk, as they must first be brought into main memory and then read. Therefore, the main goal of a Buffer Pool Manager is to reduce the number of page transfers from disk to main memory, so that most of the pages required by the query are already present in main memory.\nThe Buffer Pool Manager needs to store some additional information to manage the buffer pool.\nPage Table: The page table is an in-memory hash table used for page lookups. It contains a key-value pair where each page ID is mapped to a frame where that page is stored. If a page is needed, the buffer pool manager checks the page table first to see if the page is present, and if it is, it returns the in-memory address of the page.\nIf the page is not present in the page table, it is a page fault. The buffer pool manager first brings the page into main memory, updates the page table, and returns the page address.\nIf the page table is full, one of the pages must be evicted, i.e., one of the pages present in a frame must be replaced with the newly required page.\nNote that this page table is also present in the buffer pool and is managed by the buffer pool manager.er\nDirty Flag: For example, if a query modifies a page, in reality, it is actually modifying the page that is present in main memory, not the page that is present on the disk. So, if any changes are made to the page, the page is marked with a dirty flag. All pages that have been marked with the dirty flag are written back to the disk to persist the data.\nOutput of Pages/Blocks: As we know, pages with dirty flags should be written back to the disk. The output of pages (i.e., writing pages back to the disk) should not be done only when the page needs to be evicted, but should be done frequently or whenever eviction is needed, and writing also depends on other factors. This is to persist the data so that it is safe, and the page can be evicted directly.\nPin/Reference Counter: If a page is being accessed by a thread, the buffer pool manager should make sure that the page is not evicted from the buffer pool. To ensure this, a pin is used. If a thread is accessing a page, the page is pinned, and the buffer pool manager never evicts a pinned page. If the thread completes its operations, it unpins the page, and now the page can be evicted. A single page can be pinned by multiple threads, and a reference counter is used to count the number of threads that are currently accessing the page.\nShare and Exclusive Locks: If multiple threads are accessing a page, say one is reading and another is writing to the page, the write operation performed by a thread changes the contents of the page, which will make the contents read by another thread inconsistent. To handle this, share and exclusive locks are used. If a thread is reading the contents of a page, it obtains a shared lock on that page. A page can have multiple shared locks, as the contents of the page will not be changed, and the threads will have read the same data. If a page has an exclusive lock, the thread must wait until the exclusive lock is removed to obtain a shared lock. If a thread is writing to the page, it obtains an exclusive lock. To obtain an exclusive lock, the page should not have any locks on it, i.e., only one process is allowed to have an exclusive lock at a time.\nMemory Allocation Policies There are two types of memory allocation policies: Global Policies and Local Policies.\nGlobal Policies: These policies make decisions in such a way that the entire workload of a database benefits.\nLocal Policies: These policies make decisions in such a way that only the needs of one transaction are prioritized.\nFor example, if there are four transactions that the database is currently processing, and three of the transactions require the same page, if global policies are being used, the buffer pool manager will bring in the page that is required by most of the transactions. If the DBMS is using local policies, it might just bring in the page that is required by only one transaction, which might be selected among others by policies such as priority, FCFS (First-Come-First-Served), and others.\nBuffer Pool Optimisations Multiple Buffer Pools: A DBMS can have multiple buffer pools, each managed by its own buffer pool manager. Generally, only one type of page is stored in a buffer pool, but a buffer pool can also store pages of more than one type. For example, one buffer pool manager records pages, and another buffer pool manager stores index pages. This increases performance as each buffer pool can have its own policies for page output, page eviction, etc.\nPre-fetching: Pre-fetching means fetching pages from disk into main memory before a request to access the page has been made. The buffer pool manager can determine which pages are most used, depending on the type of workload, previous queries, and a query will be executed in many steps. The buffer pool manager can predict which pages will be requested in the near future by checking the steps being executed and bring the pages into memory before the request to access the pages is made.\nScan Sharing: When accessing records, a pointer traverses each record in a page, processing one at a time. Since a DBMS typically handles multiple queries concurrently, it\u0026rsquo;s possible that two or more queries access the same records. In such cases, the pointer traversing the records can be shared among multiple queries.\nBuffer Pool Bypass: In buffer pool bypass, some part of the buffer pool memory is specifically assigned to a query process, where the records needed by the query are directly loaded and further used. This is done to avoid the overhead of managing pages in the buffer pool, such as tracking pin/unpin pages, dirty pages, etc. This technique is generally used in large sequential scans. If these pages were normally managed by the buffer pool during a large sequential scan, most of the time would be spent evicting pages, and it would also slow down other concurrent processes.\nOS page cache Generally, the OS keeps its own cache of pages loaded into memory. As the DBMS is managing memory on its own, the pages kept in the cache by the OS become redundant. So, the DBMS uses direct I/O to avoid duplicate page caches.\nBuffer Management Policies If a query needs a page that is not currently in the buffer pool, the buffer pool manager should bring the page into memory and return the address of the frame where the page is stored. But if all frames in the buffer pool are full, then any one of the pages present in memory must be replaced to bring the needed page into memory. It is similar to OS CPU scheduling. The buffer pool manager can use policies such as LRU, FIFO, MRU, CLOCK, etc.\nLRU (Least Recently Used): LRU stands for least recently used, as the name suggests. The page that has not been accessed for a long time will be removed from memory and replaced by a new page. To handle LRU, it can be implemented using doubly linked lists (more efficient), where the recently accessed page is moved to the top, and the least recently accessed page is present at the last of the doubly linked list. Alternatively, we can maintain a table (less efficient) containing the timestamps of when the pages are last accessed and update the table whenever new accesses are made. LRU is slow as we need to update the table every time, but it is easy to implement, and it is common that the page that is not being used will most likely not be used in the future.\nFIFO (First In First Out): It is a simple policy where the page that is first brought into memory is selected for eviction when all frames are full. It is similar to the workings of a queue. Similar to LRU, FIFO can be implemented using a linked list (more efficient) or by using a table where the time when the page is loaded into memory is tracked (less efficient).\nCLOCK: Clock is an efficient but approximate implementation of a LRU policy. Here, each frame is associated with a circular data structure, which is used to store the state flag (reference bit), either 0 or 1, and a pointer, also called the clock hand, is used to traverse these flags. If its flag is 1, then 1 is set to 0. If the flag is 0, the page is selected for eviction. Whenever a new page is loaded into main memory, its reference bit/flag is set to 1. The clock hand performs the traversal around the flags whenever all frames are full and a new page is needed to be loaded into main memory.\nA Little Introduction to Indexes: Indexes are used to find the block/page in which the record is stored. For example, if we want to find a record given its ID, we can use the indexes to find the block it is present in and load the block/page into memory and read the record. It is really useful as it saves the work of traversing through every page and checking if the record we want is present there.\nHash Tables A hash table contains an array of buckets. Where each bucket can store one or more records, usually a small fixed size. It uses a hash function h(x)which takes in a key value and produces a new value ranging from 0 to B - 1, where B is the total number of buckets. The value produced by the hash function is used as an index where the key will be stored.\nFor example, consider h(x) = x % 10 , and we want to store a simple record, say “id : 10, name : cosmos”, and we are using id as the key. First, the key is passed into the hash function.\nh(10) = 10 % 10 = 0 , we get 0, we store the record in the bucket whose index is zero.\nThe average time complexity of a hash table is O(1) and the worst time complexity of a hash table is O(n).\nHash functions: An ideal hash function must be faster to compute and must uniformly distribute the keys among all the buckets, and there should be no skew in the distribution of buckets. To avoid skewing of the distribution, one must choose a good hash function, and also in hash tables, usually more buckets are allocated than the total number of keys to further avoid collisions.\nTwo types of hashing: static hashing and dynamic hashing Static hashing: In static hashing, the size of the hashing table is fixed, and the number of keys are known before hashing. The hash function stays constant and is not changed. A disadvantage of static hashing in DBMS is that the number of buckets in the hash table is fixed, and if the memory allocated to the hash table is very much, then it leads to wastage of memory, and if the memory allocated to the hash table is very little, then it leads to more number of collisions and slower lookups.\nDynamic hashing: In dynamic hashing, the size of the hash table can be changed, it can both grow and shrink. The output range of the hash function is also changed along with the size of the table. A disadvantage of a dynamic hash table is mainly about resizing of the hash table, it is an overhead as the memory should be allocated again, and every record in the hash table must be hashed again. The resizing is generally done when the load on DBMS is less.\nCollision handling A collision is said to occur when two or more keys are mapped to the same bucket index. Here, two methods can be used: open addressing and closed addressing\nClosed addressing: Here, a new overflow bucket is used to accommodate the new values in the same bucket index by chaining the buckets (overflow chaining). Closed addressing is generally used in DBMS.\nOpen addressing: Here, the size of the hash table is generally fixed (but can be resized), and no overflow buckets are used. The collisions are handled by placing the new value in some other index of the same hash table, methods such as linear probing, quadratic probing come under this method.\nStatic Hashing techniques Linear Probing Linear probing uses an open addressing approach to handle collisions in the hash table.\nInsertion: In linear probing, if a collision occurs, i.e., there is already a value stored in the bucket (A), then we check if the next bucket (A+1) is empty. If it is, we store the new value there. If not, we again check the next bucket (A+2) and so on. If it is the last bucket in the hash table, we jump to the first bucket in the hash table and continue checking until we find an empty bucket. The initial bucket index is also kept track of so that we don\u0026rsquo;t loop around the buckets.\nDeletion: Deletions are a bit more complex than insertions in linear probing. Say we want to delete a value in the bucket. We hash its key and find its index. Overflow might have occurred, so we must compare the key in the bucket with the key we want. If they are the same, we delete it. If not, we check the next bucket and so on.\nIf there are three values, and if the middle value is deleted, and if we want to get the 3rd value, we do the linear probe again and see that the next value (2nd) is empty. The algorithm might assume that there is no third value, but it\u0026rsquo;s not true. To handle this problem, we use different methods\nTombstone Approach: We store a special value in the bucket, which will indicate that an overflowed value is deleted, and there might be some more overflow values in the next buckets.\nMovement Approach: In this approach, we move all the overflowed buckets up to occupy the newly deleted bucket.\nLinear probing is not generally used in DBMS because the insertion and deletion processes are not efficient, as the number of records might change, and deletions might be performed frequently.\nRobin Hood Hashing Robin Hood is a famous character who steals from the rich and gives to the poor.\nJust like the character Robin Hood, Robin Hood hashing also steals from the rich and gives to the poor. So, how can we say that this value is rich and this is poor? This can be done using probe sequence length.\nProbe Sequence Length: Probe sequence length is the count of the number of buckets we must probe to get to the key we want. For example, say a hash function hashes a key to the index 0, but due to overflow, those buckets are already filled, and say the key is inserted at the index 2. Here, the probe sequence length (PSL) is 2. Note: If PSL is less, the element is rich; if PSL is more, the element is poor.\nInsertion: Insertion in Robin Hood hashing is similar to linear probing. Here, the PSL (probe sequence value) of each key is tracked. If we want to insert a value, we get its key, hash it, and get its index. We check if the bucket at the given index is full. If not, we just insert it there. If it is, we check the next bucket and also check the PSL values (keep in mind that now the PSL value of the key to be inserted is increased by one, as it is now one block away from its hashed index). If the PSL of the key in the bucket is greater than or equal to the PSL of the key to be inserted, we just keep checking further buckets. If the PSL of the key in the bucket is less than the PSL of the key to be inserted (the key in the bucket is considered rich, and the key to be inserted is considered poor), we swap the key present in the bucket with the new key. The old key is again reinserted using the same logic.\nCheck out this pdf to read more in-depth about Robin Hood hashing.\nCuckoo Hashing Working on Cuckoo Hashing is similar to the behavior of the Cuckoo bird, which lays its eggs in other birds\u0026rsquo; nests and, when those eggs hatch, throws out the eggs of the other birds. Similarly, here, an element occupies the slot of another element and throws out the old element.\nIt consists of multiple hash tables, each with its own hash function.\nInsertions: When inserting an element, the indexes of all the hash tables are calculated using their corresponding hash functions. If any of the buckets is empty, the new element is inserted. If not, one of the buckets is randomly selected, the new element replaces the old element, and the kicked-out element is reinserted into the hash table using the same logic. To prevent infinite loops, it allows only a limited number of reinsertions.\nDynamic Hashing Techniques Chained Hashing Chained hashing is the simplest form of hashing technique, in which a linked list is used.\nInsertions: When inserting a value, if the bucket to which the key is hashed is already full and the pointer stored is null, a new overflow bucket is allocated, and a pointer to this new bucket is stored in the old bucket that was full. Now, the new value is stored in the newly created bucket. And if there is a pointer to another bucket, and if a slot is empty in it, the value is inserted there. If even that bucket is full, and there is a pointer stored for another bucket, the other bucket is checked, and so on.\nLinear Hashing Linear hashing performs incremental growth of the hash table, where the hash table is not directly doubled in size, but a new bucket is added when an overflow occurs. It uses two hash functions and a pointer known as the split pointer to keep track of which bucket to split next.\nInsertions: Here, just like every other algorithm, we hash the key and find its hash bucket. If it is empty, we insert it there. If it is not, a new overflow bucket is added. Keep in mind that the overflow bucket can be present in linear hashing, but the number of overflow buckets is generally reduced as the algorithm proceeds. After the new overflow bucket has been added, another new bucket is added to the hash table, and a new hash function is used, say h2. Now, the values in the bucket to which the split pointer is pointing are rehashed again using the new h2 hash function, and the split pointer is made to point to the bucket.\nLookups: Lookups are a bit more complex here. Say we want to look up a value; we first hash it using the original hash function h1. If the indexed bucket is present before the split pointer, we rehash it again using the h2 hash function to get the bucket in which it is stored. And if the hashed bucket is present after the split pointer, we use the original hash function itself, so no rehashing is required.\nThe linear hashing performs splitting in a round-robin fashion and performs the splitting in the form of rounds. When a round is completed, the split pointer is reset to the 0th bucket. A round is set to be completed if all the initial buckets (those buckets that are present before the expansion) are split.\nExtensible Hashing Extensible hashing is a dynamic hashing method in which the table is grown and shrunk as requests are made. Usually, the size of the hash table is doubled when growing and halved when being shrunk. Note that even if the size of the hash table is doubled, buckets are not assigned yet, and new buckets are only created when a key is hashed to it. They are only allocated when they are needed. The method uses the first n bits of a hashed value to find the index of the key. Here, the number of first n bits used is called the count. A global count is stored for the entire hash table, and every bucket has its own local count.\nInsertions: Here, the first n bits of the value generated from hashing the key are used as the index. If there is an empty slot in the bucket, it is inserted there. If not, we first check if we want to increase the size of the hash table. This is done by checking the local count of the bucket and the global count. If both are the same or the local count is greater than the global count, then there is only one element in the hash table that points to the bucket. So, we increase the size of the hash table. We increment the global count by one (n + 1, so now the first n+1 bits are considered), effectively doubling the size of the hash table. For every two entries in the hash table, we make them point to their original bucket, and for the bucket where the overflow occurred, we create a new bucket and assign the second entry to point to the newly created bucket. Then, we retry the insertion, which will now be mostly successful.\nIf growing the hash table is not required, we just create a new bucket and assign the second entry pointing to the same bucket to point to the newly created bucket. Then, we retry the insertion, which will now be mostly successful. However, if all the values present in the bucket have the same search key, splitting will not solve the problem, so we must have some kind of way to handle large amounts of duplicate search keys.\n","permalink":"http://localhost:1313/posts/buffer-pool-manager-and-hashing/","summary":"This blog is a continuation of the database systems series, covering Lectures 5 and 6 of the Database Systems CMU 15-445/645 course.\nBuffer Pool The buffer pool is the memory present in the main memory (RAM) that is used to store pages that are most frequently used, similar to a cache. It is managed by the DBMS (buffer pool manager), because the DBMS can do a better job of managing pages than the OS.","title":"Buffer Pool and Hashing"},{"content":"This blog is a continuation of the database systems series, covering Lectures 3 and 4 of the Database Systems CMU 15-445/645 course.\nHow does a DBMS store data? A DBMS uses disks for data storage and persistence, and there are many storage devices that the database can use to store data.\nStorage Devices Storage devices are devices that are used to store data. There are mainly two types of storage devices: volatile and non-volatile.\nVolatile Storage Devices These devices are usually more expensive, faster, and have less storage capacity. Volatile storage devices require a constant supply of electricity to hold data. If the power is down, the data is completely lost.\nDevices categorized as volatile storage devices include CPU Registers, CPU Cache, and RAM.\nNon-Volatile Storage Devices Non-volatile storage devices are usually less expensive and provide far more storage capacity than volatile storage devices but are very slow. These storage devices do not require a constant supply of electricity, meaning the data remains even when the power is off.\nDevices categorized as non-volatile storage devices include SSDs, HDDs, DVDs, etc.\nHow does a DBMS store data on disks? A DBMS stores databases in a single file or multiple files with hierarchical relationships, or it can use one file per relation, etc.\nHow are files structured? Each file is divided into a bunch of fixed-sized chunks known as pages. The size of these pages varies from one DBMS to another. Storage is also divided into pages known as hardware pages, where the storage device can only guarantee an atomic write of a size equal to that of hardware pages. For instance, if the hardware page size is 4KB, then 4KB of data is written all at once or not written at all. There is no intermediate state.\nHow are pages structured? There are several ways to structure a page, depending on whether the size of the records to be stored is fixed or variable. Records are stored in pages. For simplicity, the size of the record is generally limited to that of the page, and a record does not span multiple pages.\nHow are pages structured if the size of the records is fixed?\nRecords can be stored sequentially, one after another, just like an array. When the size of the records is not a multiple of the page size, the last part of the page is left unused to ensure that the record stays in one page instead of spanning multiple pages.\nInsertion can be costly as one must parse the entire page to find an empty location. To handle this, the first empty record stores a pointer to the next empty record, and so on, forming a linked list of empty records known as a free list.\nHow are pages structured if the size of the records is variable?\nThe main problem with variable-sized records is that maintaining the free list is not helpful as the size of the record can change from one to another. If the size of an empty record found is 50 bytes and we want to insert a record of 56 bytes, which is not possible, to handle this slotted pages are used.\nIn variable-length records, the records have two parts. The first part has a fixed length and is the same for all the records of the same relation. The second part is the variable-length data.\nThe first part of the record consists of many pairs of values, where each pair stores the starting address and length of each attribute’s data. In pages, slots are used to track the records. A slot merely stores the starting and ending address of a record. These slots are present at the start of the page and grow towards the end of the page. Here, records are stored at the end and grow towards the start of the page. When both the slots and pages meet, we know that the page is full.\nLog structure of pages\nIn this form, relations are stored as logs. For example:\nINSERT A; INSERT B; INSERT C; UPDATE A; DELETE B; Here, writing to the database is extremely fast, but reading is very slow because one must parse the entire log to get the desired record.\nGenerally, logs are added from start to end, and reading is done from end to start.\nReading from end to start can be helpful. Say we want to read record B. If we were reading from start to end, we would have to parse the entire file, but if we read from end to start, we can stop parsing as soon as we find the INSERT B; log.\nHow Are Tuples/Records Structured? Tuples are simply a sequence of bytes that make sense to the DBMS. They generally contain:\nTuple header:\nThe tuple header stores metadata related to the tuple, such as the number of null attributes, the size of the tuple, the size of the attributes, etc.\nTuple Data:\nThe actual data for the attributes. These attributes are generally stored in the order specified in the table. For instance, if the table is defined as follows:\nCREATE TABLE table_name ( column1 datatype, column2 datatype, column3 datatype, .... ); Then tuples will be stored in the following order:\nUnique Identifier: Each tuple in the database is assigned a unique identifier. The most common is a combination of page ID and an offset or slot.\nWhat If the Size of a Tuple Is Greater Than the Size of a Page?\nSometimes, we need to store large files like images or videos. In cases where the size of a record exceeds the page size, the DBMS uses special pages known as overflow pages, and a pointer to the overflow page is stored in the record. If the data does not fit in a single record, it can span multiple overflow pages, or it can be stored in an entirely new file outside the database. A pointer to that data file is then stored in the record. However, some features offered by the DBMS may not apply to this external data file.\nHow are records organized in files? So, how does the DBMS know which block to store the record in? There are multiple ways to organize records in files:\nHeap File Organization:\nIn heap file organization, records are stored in any available free space in the files, and there is no specific ordering of the records. Once stored, they are generally not moved. To find free space quickly, the DBMS uses an array of bytes where each byte corresponds to a page and stores a number representing the fraction of space that is free in that page. For example, if the page size is 8 bytes and the value stored is 6, then a 6/8th fraction of space is free in that page. This array of bytes is known as the free space map. Sequential File Organization:\nIn sequential file organization, records are stored in a specific order based on a search key. Sequential file organization is useful for displaying and computing queries. Each record stores a pointer to the next record, which comes after it sequentially. Here, the records are also stored as physically close as possible to reduce disk read time. For instance, say we have a relation stored in sequential file organization, and the search key is Id. If we want to insert a new record, \u0026ldquo;D,\u0026rdquo; whose Id is 4, the DBMS first locates the record that comes before Id 4, which is 3. if there is free space in the same block 4 is stored there; if not, 4 is stored as a new overflow block, and 3 points to that overflow block. Multitable Clustering File Organization: In multitable clustering file organization, records belonging to different relations are stored on a single page or block. This form of file organization helps when handling JOIN queries. However, reading records that belong to a single relation but are stored in multitable clustering can be slow, as the records will be spaced far apart. B+ Trees File Organization: B+ tree file organization uses a B-tree data structure to organize files. Here, the B+ tree provides very fast insert, read, delete, and update operations. The time complexity of B+ trees is O(log(n)) for both insertion and deletion. B+ trees are like evolved versions of binary trees. Hashing file organization In hashing file organization, a key is hashed to determine the location where the record should be stored. Hashing allows for both faster insertions and faster lookups, but the hash function used must be fast and distribute data uniformly. Hashing file organization is not suitable for OLAP-type workloads. Why Does the DBMS Manage Memory Better Than the OS? The DBMS can rely on the OS to load pages into memory when required. While this can be helpful, the DBMS can handle memory much better because it knows much more about the database files than the OS does. The DBMS can use its knowledge of database files to prefetch pages, search for pages more effectively, and so forth.\nIf the DBMS relies on the OS for managing memory, it can be dangerous because if any changes are made to a page, the OS can remove the page from memory without writing it back to disk, causing data loss since the DBMS has no control over the OS page cache.\nThus, many DBMSs have their own Buffer Pool that manages the loading of data to and from disk and memory. If an execution engine requests a specific page to execute a query, it makes a request to the Buffer Pool, which then loads the required page into memory and returns a pointer to it.\nSystem Catalog The DBMS needs to store information such as relations present in a database and files associated with the databases. The DBMS maintains a catalog that stores all the required data. Some additional data, such as the number of records, distinct records, and the sum of attributes, can also be stored using the DBMS\u0026rsquo;s own functionalities. However, metadata required for the catalog itself (a catalog of catalogs 😅) is also embedded in the software.\nWorkloads Workloads refer to the general types of requests that the DBMS should process.\nThere are several different types of workloads, including Online Analytical Processing (OLAP), Online Transactional Processing (OLTP), Decision Support System (DSS), and Business Intelligence (BI).\nThe three main types of workloads are:\nOLTP(online transactional processing) Most of the query requests received by the database are short and mostly require accessing only one record. They usually involve insert, delete, and update operations. The time taken to process a request is minimal. OLAP(online analytical processing) Most of the query requests involve accessing most or all of the records in the table. These requests are usually analytical in nature and can be used to generate reports, analyze trends, etc. They usually take a long time to process, often hours. HTAP(Hybrid transaction + analytical processing) Most queries received by the DBMS are usually a mix of both OLAP and OLTP queries. Storing Records Based on DBMS Workload N-Ary Storage Model (NSM) In this storage model, the records are stored normally, with a record containing the data of all attributes. This type of storage model is usually helpful for OLTP workloads, as the entire record can be modified in one place. However, it is not preferred for OLAP workloads because if a query requires only a few attributes of a relation, the entire page is loaded into memory, which contains attributes that are not needed. This wastes buffer pool memory and disk time (more time is taken to load).\nDecomposition Storage Model (DSM) In this storage model, only specific attributes are stored on a page. For example, if a relation has attributes A, B, and C, all values of attribute A are stored sequentially on one page, B on another, and so forth. So, when a query needs specific attributes of a relation, only those can be loaded into the buffer pool.\nThis storage model is suitable for OLAP workloads but not for OLTP, as all attributes of a record are stored on different pages. If we need to update a single record, we must update multiple pages.\nThe DBMS can use a fixed-length approach, where all attributes of the same record are stored at the same offset. So, if we want to access an attribute of a record, we can jump to that specific offset.\nAnother approach is the embedded tuple approach, where every tuple is embedded with a primary key of the corresponding tuple. This wastes memory by storing primary keys and is not widely used.\n","permalink":"http://localhost:1313/posts/database-storage/","summary":"This blog is a continuation of the database systems series, covering Lectures 3 and 4 of the Database Systems CMU 15-445/645 course.\nHow does a DBMS store data? A DBMS uses disks for data storage and persistence, and there are many storage devices that the database can use to store data.\nStorage Devices Storage devices are devices that are used to store data. There are mainly two types of storage devices: volatile and non-volatile.","title":"Database Storage"},{"content":"I am currently studying database systems. This blog summarises lectures 1 and 2, as well as the SQL homework from the Database Systems CMU 15-445/645 course.\nIt\u0026rsquo;s fascinating to realize the extent to which we interact with database systems in our daily lives. They\u0026rsquo;re not just confined to large-scale applications but are also embedded in our browsers and mobile devices. Before the advent of database management systems, data management was complex, with a strong coupling between the data and its management. This required constant re-implementation for different applications.\nTo address this issue, Edgar F. Codd introduced the concept of the relational model, one of many data models including graph, network, document, and arrays/matrices.\nA data model is a collection of concepts for describing the data in a database. The relational model is an example of a data model.\nThe most widely used database system is the relational database, which was developed based on Edgar F. Codd\u0026rsquo;s proposals. In this system, data is stored in a relation (or table), and each record (row) or tuple contains the data. A relation consists of attributes (columns).\nSo, how do we communicate with a DBMS?\nWe use Data Manipulation Languages (DMLs) which can be procedural or non-procedural (declarative). Procedural languages specify both what we want and how we want the data, while non-procedural languages only specify what we want, leaving the retrieval method up to the DBMS.\nRelational algebra, is like a math of databases and uses special symbols to represent specific operations on relations. Each operation takes one or more relations as input and produces a new relation as output.\nThere are several types of operations in relational algebra:\nSelect: This operation takes a relation and outputs a subset of tuples that satisfy a certain condition or predicate. Projection: It outputs a relation with tuples containing only specified attributes. Union: This operation outputs a relation containing all tuples appearing in at least one of the input relations. Intersection: It outputs a relation containing all tuples appearing in both input relations. Difference: This operation outputs a relation containing all tuples appearing in the first relation but not the second. Product: It outputs a relation containing all possible combinations of tuples from the input relations. Join: This operation outputs a relation where two relations are joined on a specific attribute. SQL is a non-procedural language, that is we only say what we want, and how? is managed by the database, there are many SQL database systems such as postgreSQL, MySQL, sqlite, all the SQL database are mostly similar but there is a difference in the methods offered and how the database system manages different types, a database system is said to support SQL if it implements SQL 92,\nhere are my answers for the assignment along with some expiation\nQ1\nSELECT CategoryName FROM Category ORDER BY CategoryName; -- this command just selects the CategoryName attribute from Category Relation -- and they tuples are ordered by the category name in ascending order Q2\nSELECT DISTINCT ShipName, SUBSTR(ShipName, 0, INSTR(ShipName, \u0026#39;-\u0026#39;)) FROM \u0026#34;Order\u0026#34; AS o WHERE o.ShipName LIKE \u0026#34;%-%\u0026#34; ORDER BY o.ShipName; -- this command selects shipName attribute and -- all the characters preceding hypen \u0026#39;-\u0026#39;, this is achived -- by using the INSTR string function, which can be used to -- get the position of a character in a string, -- and SUBSTR function is used to get a substring of a string -- LIKE is used for wild card matching of string, where \u0026#39;%\u0026#39; any characters -- including an empty string \u0026#39;\u0026#39; -- the retreived tuples are ordred by the shipName Q3\nSELECT Id, ShipCountry, CASE WHEN (ShipCountry LIKE \u0026#39;USA\u0026#39;) OR (ShipCountry LIKE \u0026#39;Mexico\u0026#39;) OR (ShipCountry LIKE \u0026#39;Canada\u0026#39;) THEN \u0026#39;NorthAmerica\u0026#39; ELSE \u0026#39;OtherPlace\u0026#39; END FROM \u0026#34;Order\u0026#34; WHERE Id \u0026gt;= 15445 ORDER BY Id LIMIT 20 -- this command selects the Id and ShipCountry attribute from \u0026#34;Order\u0026#34; relation -- CASE is similar to if else in general progrmming, it is used to check if the -- ShipCountry is \u0026#39;USA\u0026#39; or \u0026#39;Mexico\u0026#39; or \u0026#39;Cannada\u0026#39;, if it is then the value \u0026#39;NorthAmerica\u0026#39; -- is stored in tuple else \u0026#39;OtherPlace\u0026#39; is stored in the tuple -- again ORDER BY is used to order the typle according to their Id -- WHERE Id \u0026gt;= 15445 is used to select only the tuples having Id more then 15445 -- LIMIT is used to limit the output to only 20 tuples(rows) Q4\nWITH orderDelay (ShipId, delay) AS ( SELECT ShipVia, CASE WHEN ShippedDate \u0026gt; RequiredDate THEN \u0026#39;late\u0026#39; ELSE \u0026#39;ontime\u0026#39; END AS delay FROM \u0026#34;Order\u0026#34; ) SELECT CompanyName, ROUND( 100.0 * COUNT( CASE WHEN od.delay = \u0026#39;late\u0026#39; THEN 1 ELSE NULL END ) / COUNT(*), 2 ) AS delayPercentage FROM orderDelay AS od, Shipper WHERE od.ShipId = Shipper.Id GROUP BY ShipId -- WITH AS statement is used to create an temprovary table -- which has ShipVia attribute stored as ShipId and it is calcuated if the -- order was late using ShippedDate and RequiredDate attributes and stored -- as delay attribute in orderDelay table -- were are SELECTing CompanyName and calculating the percentage of late orders -- and rounding them to 2 decimal places using ROUND function and storing it as -- delayPercentage attribute, we are using both the Shipper, and the newly created -- orderDelay Relation to calculate the delayPercentage Q5\nSELECT CategoryName, COUNT(*) AS numberOfProducts, ROUND(AVG(UnitPrice), 2), MIN(UnitPrice), MAX(UnitPrice), SUM(UnitsOnOrder) FROM \u0026#34;Product\u0026#34; AS p JOIN \u0026#34;Category\u0026#34; AS c ON p.CategoryId = c.Id GROUP BY c.Id -- here we are joining two relations \u0026#34;Product\u0026#34; and \u0026#34;Category\u0026#34; based on the CategoryId -- and calculating umberOfProducts using COUNT, average UnitPrice using AVG, minimum -- UnitPrice using MIN and maximum UnitPrice using MAX and total orders using SUM -- aggrigate functions, -- here the GROUP BY is used to group the tuples having same CategoryId, hence the -- aggrigate function performs operations on the gorups of tuples having same -- CategoryId Q6\nWITH discounted (orderId, productId) AS ( SELECT OrderId, ProductId FROM OrderDetail as od, Product as p WHERE od.ProductId = p.Id AND p.Discontinued = 1 ) SELECT ProductName, CompanyName, ContactName FROM ( SELECT *, RANK() OVER( PARTITION BY d.ProductId ORDER BY o.OrderDate ) as rank FROM \u0026#34;Order\u0026#34; as o, discounted as d WHERE o.id = d.orderId ) JOIN Customer ON CustomerId = Customer.Id JOIN Product ON Product.id = ProductId WHERE rank = 1 ORDER BY ProductName -- here we are getting a relation of all the products, which are discounted -- and storing the obtained tuples in discounted relation -- we use inner Query to add a new attribute of rank based on -- when the companies have ordered the product, this is acheived -- using the RANK window function and PARTITION BY and ORDER BY -- so tuple having RANK 1 ordered the product first -- finally in the outer query we only select the companies which -- brough the product first Q7\nSELECT o.Id, OrderDate, LAG(OrderDate, 1, 0) OVER( ORDER BY OrderDate ) AS PreviousOrderDate, ROUND( julianday(OrderDate) - julianday( LAG(OrderDate, 1, 0) OVER( ORDER BY OrderDate ) ), 2 ) AS \u0026#39;Lag\u0026#39; FROM \u0026#34;Order\u0026#34; AS o, Customer AS c WHERE o.CustomerId = c.Id AND o.CustomerId = \u0026#39;BLONP\u0026#39; ORDER BY OrderDate LIMIT 10 -- here we mainly use the LAG function to get the value of the -- preceding tuple, LAG should almost always be used with ORDER BY -- becuase if we didn\u0026#39;t use the ORDER BY the ouput can be randomized -- we are caclucating the time difference between two orders using -- the LAG aggrigate function and julianday function as ouputing it as -- \u0026#39;LAG\u0026#39; attribute -- we are limiting the output to 10 tuples using LIMIT Q8\nWITH expendeture(Id, expen) AS ( SELECT CustomerId as Id, SUM(UnitPrice * Quantity) AS expen FROM \u0026#34;Order\u0026#34; AS o JOIN OrderDetail AS od ON o.Id = od.OrderId GROUP BY CustomerId ) SELECT CASE WHEN ( e.Id NOT IN ( SELECT Id FROM Customer ) ) THEN \u0026#39;MISSING_NAME\u0026#39; ELSE ( SELECT CompanyName FROM Customer WHERE e.Id = Id ) END AS \u0026#39;CompanyName\u0026#39;, e.Id AS \u0026#39;CustomerId\u0026#39;, ROUND(e.expen, 2) AS \u0026#39;Expences\u0026#39; FROM expendeture AS e ORDER BY e.expen -- here we create a new table which contains the customerId and -- customer expences as attributes then in the outer query -- we are using case to check if the customer in present in the customer table -- if present we use the CompanyName else we use \u0026#39;MISSING_NAME\u0026#39; -- we round the expences of company to -- two decimal places using ROUND function Q9\nselect RegionDescription, FirstName, LastName, BirthDate FROM ( select *, RANK() OVER( PARTITION BY r.id ORDER BY BirthDate DESC ) AS rank from Employee as e JOIN EmployeeTerritory as et ON e.id = et.EmployeeId JOIN Territory as t ON et.TerritoryId = t.Id JOIN Region as r ON t.RegionId = r.Id ) WHERE rank = 1 GROUP BY Id ORDER BY RegionId -- here we are using the inner query, to rank the employee -- according to their birthdate -- and in the outer query we are selecting the employee whose rank is 1 -- and ordering the output by RegionId Q10\nWITH productNames(pname, id) AS ( SELECT ProductName, ROW_NUMBER() OVER( ORDER BY ProductId ) FROM \u0026#34;Order\u0026#34; as o JOIN OrderDetail as od ON o.id = od.OrderId JOIN Product as p ON od.ProductId = p.Id JOIN Customer as c ON c.id = o.CustomerId WHERE julianday(substr(OrderDate, 0, instr(OrderDate, \u0026#34; \u0026#34;))) = julianday(\u0026#39;2014-12-25\u0026#39;) AND CompanyName = \u0026#39;Queen Cozinha\u0026#39; ORDER BY ProductId ) SELECT * FROM productNames -- here we are using julianday to get the day -- order which are made on the required day -- and were are selecting only the required companyName -- and ordering the orders according to their productId i am still trying to wrap my head around the recursion in SQL, so my 10th answer is incomplete,\nhere are some sources that help me complete this assignment\nhttps://www.sqlitetutorial.net/\nhttps://www.youtube.com/@AlexTheAnalyst\n","permalink":"http://localhost:1313/posts/database-systems-1/","summary":"I am currently studying database systems. This blog summarises lectures 1 and 2, as well as the SQL homework from the Database Systems CMU 15-445/645 course.\nIt\u0026rsquo;s fascinating to realize the extent to which we interact with database systems in our daily lives. They\u0026rsquo;re not just confined to large-scale applications but are also embedded in our browsers and mobile devices. Before the advent of database management systems, data management was complex, with a strong coupling between the data and its management.","title":"Intro to Database Systems"},{"content":"What is a webhook?\nA webhook is an HTTP POST request that is made when a change or event happens in a system. For example, this could occur when the system starts, stops, or when certain conditions are met. This HTTP POST request contains information regarding the change, which programmers can use to initiate actions.\nConsider GitHub as an example. When a push event occurs (which is a webhook event), GitHub sends a POST request to the payload URL. This request contains the required information.\nWhat is a webhook tester?\nAs you saw with GitHub, it needs a \u0026ldquo;payload URL\u0026rdquo; to send the POST request to. This payload URL must be publicly available. If you\u0026rsquo;re building a program that receives POST requests and performs actions based on them, it needs to be able to receive the request first. To achieve this, you\u0026rsquo;d typically need a publicly available URL. However, if you\u0026rsquo;re just starting development, you might not want to purchase your own domain. This is where a webhook tester comes in. A webhook tester provides a temporary URL that you can use during development, eliminating the need to set up your own domain.\nWebhook Tester Design\nI am building a webhook tester using Golang. Here\u0026rsquo;s an overview of how it works:\nThere are two main parts: a server and a client, referred to as whserver and whclient, respectively. The whclient program is used by someone who is developing a program (program A) that performs an action when it receives a webhook POST request. Whclient is a binary file that runs on a client machine. It takes the port number as an argument, which is the port on which program A is running. When the client runs the binary file, it generates a random URL that is publicly available. This URL can be used as a payload URL to which the POST request can be made. The whserver listens for the POST request. Upon receiving it, the server encodes the request and transmits it to the whclient through a WebSocket connection. This WebSocket connection is established when the whclient binary file is initially run. The whclient rebuilds the request and makes the same request to the locally running program, simulating as if program A received the request directly. How it Works\nThere are three main packages: CLI (command-line interface), server, and serialize.\nServer Package\nThe server has two main functions:\nForwarding the received HTTP POST request to the corresponding client. Adding new clients. Adding New Clients\nThis is done using a type called subdomains, which is a map that maps a URL to an HTTP handler function. The key is the URL, and the value is the handler function.\nWhen the client binary makes a request to the /ws endpoint, the NewWebHookHandler is used to create a new mux (multiplexer). When a new request is received at the /ws endpoint, a new WebSocket connection is created between the client and the server, and a new random URL is generated. Then, the AddNewClient(u string, ws *websocket.Conn) method is called. Here, u is the randomly generated URL, and ws is a pointer to the established WebSocket connection.\nThe AddNewClient function creates a new handler function that uses the ws connection to send messages to the client. It also creates a new entry in subdomains. This establishes a WebSocket connection with the client, associates a randomly generated URL with it, and creates a handler function to communicate with the client.\nForwarding Messages to Clients\nWhen the server receives any POST request at the root endpoint, it calls the ServeHTTP function, which is a method of subdomains. This gives the server access to the URLs and corresponding client handlers. The ServeHTTP method of the subdomains type iterates through the URLs one by one and checks if the received request\u0026rsquo;s URL matches any of them. If a match is found, it calls the corresponding handler function, which encodes the received request and sends it to the client through the WebSocket connection. This way, the client receives the HTTP request.\nCLI Package\nThe client has two main purposes:\nEstablishing a WebSocket connection with the server and displaying the URL received from the server. Making the same request to the locally running program. The client is primarily built around the client structure.\nClient Structure\ntype client struct { URL string Conn *websocket.Conn httpClient *http.Client } The client structure defines the following fields:\nURL: This string stores the URL generated by the server. Conn: This pointer holds the established WebSocket connection with the server. httpClient: This is used to make HTTP requests to the local program. Client Functionality\nThe client establishes a WebSocket connection with the server by calling the NewConn function:\nfunc NewConn(wsLink string) *websocket.Conn { ws, _, err := websocket.DefaultDialer.Dial(wsLink, nil) if err != nil { log.Fatalf(\u0026#34;error establishing websocket connection: %v\u0026#34;, err.Error()) } return ws } This function attempts to connect to the provided URL (wsLink) and returns a pointer to the established WebSocket connection (ws). If an error occurs during connection establishment, it\u0026rsquo;s logged using log.Fatalf.\nOnce connected, the client retrieves the generated URL from the WebSocket connection and creates a new http.Client instance. Finally, it enters a loop to listen for incoming messages on the WebSocket connection using the Listen method:\nfunc (c *client) Listen(w io.Writer, fields []string, urlstr string) error { data, msgType, err := read(c.Conn) if err != nil { return err } if msgType == websocket.TextMessage { fmt.Fprint(w, \u0026#34;\\n\u0026#34;+string(data)) } else if msgType == websocket.BinaryMessage { req := serialize.DecodeRequest(data) fmt.Fprint(w, readRequestFields(fields, *req)) req.URL, _ = url.Parse(urlstr) req.RequestURI = \u0026#34;\u0026#34; _, err := c.httpClient.Do(req) // Send the decoded request to the local program if err != nil { log.Fatalf(\u0026#34;\\ncli could not forward message to local server: %v\u0026#34;, err) } } return nil } The Listen method first reads data, message type, and any potential errors from the connection. It then handles different message types, on receiving a binary message it decodes it, and makes the same request to the locally running program\n","permalink":"http://localhost:1313/posts/webhook-tester/","summary":"What is a webhook?\nA webhook is an HTTP POST request that is made when a change or event happens in a system. For example, this could occur when the system starts, stops, or when certain conditions are met. This HTTP POST request contains information regarding the change, which programmers can use to initiate actions.\nConsider GitHub as an example. When a push event occurs (which is a webhook event), GitHub sends a POST request to the payload URL.","title":"WebHook Tester"},{"content":"TCP stands for Transmission Control Protocol. It\u0026rsquo;s built upon the IP layer and provides features like reliable packet transmission, flow control, congestion control, and congestion avoidance.\nWhy is TCP the most used protocol?\nTCP is widely used because of the features it offers, such as reliable packet transmission, flow control, and others. Reliable transmission ensures data arrives complete and in order, making it crucial for applications like file transfers, email, and web browsing.\nWhat does reliable packet transmission mean?\nWhen packets travel over the internet, they can get lost or corrupted. Reliable transmission guarantees that all packets arrive at their destination complete and in the correct order. If a packet is lost or corrupted, TCP detects it using checksums and requests the sender to resend it. This ensures accurate and complete data exchange.\nWhat is flow control?\nThink of flow control like regulating water flow from a large tank to a small cup. If the water flows too fast, the cup overflows. Similarly, Our computer will have some memory buffer which is used to store the incoming packers before processing, when the server keeps on sending the data and when our ability process them in less, packets will start accumulating in the memory buffer and soon will lead to packet loss due to buffer overflow, to avoid this TCP will keep track of a variable known as receive window (rwnd) both the server and client will have their own receive window, and these are initially exchange during the three-way handshake, when there is a change in rwnd it can again be exchanged between the server and client to keep up with the new requirements\nWhat is congestion control?\nTwo main factors determine your internet speed: bandwidth and receive window. Flow control takes care of the receive window, but what about bandwidth? Imagine you have a 30Mbps connection and the server sends data at 100Mbps. This creates network congestion because your connection can only handle 30Mbps, leading to packet loss. So, how does the server handle this?\nThe server tracks another variable called \u0026ldquo;congestion window size (cwnd)\u0026rdquo; – the amount of data it can send at once. Unlike the receive window, the cwnd is not shared, and for Linux systems, it starts at 4kb. TCP dynamically adjusts the cwnd based on acknowledgments (ACKs) from the client. It increases the cwnd if all packets are received and decreases it if there\u0026rsquo;s any packet loss. There are various congestion control algorithms like Slow Start, Congestion Avoidance, and Fast Retransmit that utilize ACKs to determine the optimal cwnd value, ensuring efficient data transfer without overwhelming the network.\nBandwidth Delay Product (BDP)\nIt ****is the product of a connection\u0026rsquo;s bandwidth and its end-to-end delay (also known as round-trip time). It indicates the maximum amount of unacknowledged data that can be in transit without overwhelming the network or receiver\u0026rsquo;s buffers.\nThe maximum amount of unacknowledged data in transit is limited to the minimum of the receive window size (RWND) and congestion window size (CWND). This is because:\nIf CWND were greater than RWND, the sender might transmit more data than the receiver can handle, leading to buffer overflow and packet loss. If RWND were greater than CWND, the sender might not fully utilize available bandwidth, potentially leading to underutilised links. therefor even if we have sufficient bandwidth our internet speed can be limited due to our receive window\n","permalink":"http://localhost:1313/posts/tcp/","summary":"TCP stands for Transmission Control Protocol. It\u0026rsquo;s built upon the IP layer and provides features like reliable packet transmission, flow control, congestion control, and congestion avoidance.\nWhy is TCP the most used protocol?\nTCP is widely used because of the features it offers, such as reliable packet transmission, flow control, and others. Reliable transmission ensures data arrives complete and in order, making it crucial for applications like file transfers, email, and web browsing.","title":"TCP"},{"content":"How information is sent across the internet? Internet is a network of networks, billions of computers interconnect make the internet, information is sent across the internet in the form of packets, packets can be thought of a little containers of data, if you want to share an image to your friend, first the large image is broken down into number of packets, and these packets are transferred to your friend’s computer, in your friends computer they are reordered and assembled together(not necessarily) to form the image\nWhat are packets? Packets in the internet closely resembles the a post in real life, just like the letter in the post they have some digital data to carry and like the to and from addresses on the envelope, packets have an IP Header, IP Header is like the address of the houses, it contains source and destination IP of the computers along with other stuff\nHow do these packets actually travel? Packets travel from hopping from one router to another, when a package is sent it fist goes to router which is provided by ISP(internet service provider) to which we are connected to, and then it goes from one router to another, how does the router decide on which direction to send the packet?, router does that from looking at the packet header, which contains the destination IP address, then it looks at a table(routing table), table contains directions and corresponding destination IP address, and now it directs the packets in the correct direction, the router also considers other factors such as network traffic etc\nWhat is a protocol? Protocols are a set of rules that are agreed upon by the computers, which facilitate the communication between them, just like how two persons should know the rules of the language to communicate in that language.\nThere are many protocols some of them are IP, TCP, UDP, HTTP\nLets take about each of them now\nIP (internet protocol): IP stands for internet protocol, IP can be thought of as a set of rules by which the packets are sent and received over the internet, all the devices which use IP has a unique IP address which is used by other devices to identify that device and also by routers to direct the packets to it. IP takes care of identifying the devices, routing the packets and breaking down and reassembling the packets into larger files\nTCP (transmission control protocol): TCP operates on a layer above IP, TCP takes care of the things like inorder delivery of the packets, TCP maintains a reliable communication, that TCP makes sure that all the packets are received and they are received in order, TCP makes request to the sender for the missing packets if there are any, TCP also takes care of the thing like flow control and congestion control, flow control and congestion control govern the rate of transmission of packets.\nUDP (user datagram protocol): UDP is used if we want to make faster communication, UDP doesn’t guarantee the packet delivery and also doesn’t deliver the packets in order, but transmission is way faster then TCP and is generally used for online game, live streaming and voice calls, the terms datagrams and protocols are used interchangeably but the key difference is that we use the terms datagram when there is un guaranteed delivery of the packet\nHTTP (hypertext transfer protocol): HTTP is a that governs the data transfer, it is primary used to transfer text data but images, audio and videos can also be transferred, it is based on response request model, where client makes a request to the server and the server responds back, it provides different methods for different types of request for example GET to request data, POST to send the data, PUT to change the existing data\n","permalink":"http://localhost:1313/posts/internet/","summary":"How information is sent across the internet? Internet is a network of networks, billions of computers interconnect make the internet, information is sent across the internet in the form of packets, packets can be thought of a little containers of data, if you want to share an image to your friend, first the large image is broken down into number of packets, and these packets are transferred to your friend’s computer, in your friends computer they are reordered and assembled together(not necessarily) to form the image","title":"internet basics"},{"content":"event modelling What is event modeling?\nEvent modeling is a simple technique used to design a system. It generally takes relatively little time to learn and get started. In event modeling, we use sticky notes to write things down.\nSticky note colors and their purposes:\nOrange sticky notes: Represent events. We write down the event occurrence in the past tense. Events are generally triggered by the UI or by some external API. Examples: Position updated, request received, response sent Blue sticky notes: Represent commands. We write down the command as an assertive sentence. A command is something that modifies the system state and is caused to happen. Examples: Update player health, send request Green sticky notes: Represent views. A view is like a snapshot of the current system state. It can be used to display the current state of the system to the user and can also be used by other system applications. Examples: Display player\u0026rsquo;s health Let\u0026rsquo;s try to create an event modeling diagram for a simple fitness app that we will be making.\nLet\u0026rsquo;s start with what we want our app to do:\nDisplay today\u0026rsquo;s to-do exercises Send reminder notifications to exercise Keep track of streaks Now that we know what our simple app should do, let\u0026rsquo;s think about the events. Here are a few that I came up with:\nRequested today\u0026rsquo;s exercises Completed today\u0026rsquo;s exercises Alarm set Notification sent Requested current streak Set time reached Now that we have our events, let\u0026rsquo;s see how these events can be triggered. We use simple wireframes or actual screenshots of the app (if it\u0026rsquo;s already developed) to represent the UI. We align the UI wireframe with the event that will be triggered by user interaction.\nNow, let\u0026rsquo;s add the blue sticky notes for the commands that need to be executed in order to trigger the events. Commands generally act as inputs from the user that change the state of the system\nAnd now, let\u0026rsquo;s also include green sticky notes, which are generally used to display the state of the system. You can also use gears or other diagrams to represent work done by the API used by our app. Here, I have represented it as a cloud as a process.\nto know more about event modelling see: https://eventmodeling.org/posts/what-is-event-modeling/\nWhy we need to use event modelling?\ni feel like event modelling helps us understand the overall working of the system better\nthe idea can be communicated more effectively to the non technical people like UI\\UX designers and more people can contribute actively to the designing and features of the system\nevent modelling make building CQRS based systems easier as there is a clear separation between the query an view in the system\nWhat is CQRS?\nCQRS stands for Command Query Responsibility Segregation. It is a design pattern that separates the read model from the write model. This means using one model for querying data and a separate model for updating data (including creates, updates, and deletes).\nWhen should we use CQRS?\nCQRS is generally used when there is a nonuniform distribution between queries to the database and updates to the database. When we have a single model for basic CRUD (Create, Read, Update, Delete) operations, scaling is not very efficient. Having two different models helps us scale the query model and update model independently, which makes our application more efficient. Proper scaling can even be done if we use different databases for reads and updates, but we need to make sure that they are in sync.\nThe CQRS model is also used when the business logic becomes more complex for basic CRUD operations.\nThings to keep in mind before implementing CQRS:\nImplementing CQRS can get complex, and if we are using separate databases for reads and updates, we need to maintain consistency between the databases. It is preferred not to use CQRS when the business logic is simple and simple CRUD models do the job.\nsources:\nhttps://eventmodeling.org/posts/what-is-event-modeling/\nhttps://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs\nhttps://martinfowler.com/bliki/CQRS.html\n","permalink":"http://localhost:1313/posts/event-modelling/","summary":"event modelling What is event modeling?\nEvent modeling is a simple technique used to design a system. It generally takes relatively little time to learn and get started. In event modeling, we use sticky notes to write things down.\nSticky note colors and their purposes:\nOrange sticky notes: Represent events. We write down the event occurrence in the past tense. Events are generally triggered by the UI or by some external API.","title":"event modeling"},{"content":"Part 1: Introduction This is an article about my experience completing \u0026ldquo;Go with Tests.\u0026rdquo; I wanted to learn a back-end programming language for web development, so after completing CS50, I first got started with web development from The Odin Project. However, I only did a few lessons and wasn\u0026rsquo;t very consistent. Additionally, I felt like every three out of five engineering college students were learning the MERN tech stack, so I wanted to do something different.\nI first stumbled across Go from ThePrimeGen video, which led me to boot.dev, where I learned about Go. Intrigued by this mix of C and Python, I discovered the free GitBook \u0026ldquo;Learn Go with Tests\u0026rdquo; and decided to give it a serious try and actually complete it.\nIt\u0026rsquo;s funny that I got stuck in the first lesson itself. I had dual-booted my laptop and was using Ubuntu for programming, but I couldn\u0026rsquo;t install any packages. Even though the commands ran, I couldn\u0026rsquo;t see the files in the /bin directory. The solution was simply to change the permissions of the Go directory, but it took me quite a while to figure that out.\nBefore \u0026ldquo;Go with Tests,\u0026rdquo; I had zero knowledge of TDD or how software applications were built and deployed in the real world. The first few lessons, until maps, were quite straightforward. They covered basic syntax and built the habit of using TDD to solve problems. Did you know there are no while or do while loops in Go? That surprised me! Slices and interfaces were new concepts for me, so I needed to do some YouTube watching and Googling to understand them. LLMs like Bard and ChatGPT were also a big help. When I didn\u0026rsquo;t understand a concept or got stuck, I would ask them \u0026ldquo;Explain {topic} in simple terms.\u0026rdquo; Then, I would watch videos or read articles to try to understand the concepts and write a detailed message explaining what I didn\u0026rsquo;t understand and asking if my understanding was correct. This method helped me a lot, and I think you should give it a try.\nPart 2: Testing Fundamentals Things got a bit harder with the testing fundamentals. I had no idea what acceptance tests were, but after watching some videos and doing some back-and-forth with LLMs, I got to know them better. In short, they are tests written to check the application\u0026rsquo;s working from the user\u0026rsquo;s perspective. I also learned how Docker is used to test the application in different environments and about the different types of test doubles that can be used. Most of the chapters used HTTP in the lessons, which helped me better understand how Go works with HTTP.\nPart 3: Building an Application I think building an application was smooth sailing. Most of the things used were covered previously, and I was keeping track of things and resources using Notion. If I forgot something, I would go back and do a quick read. Even then, if I didn\u0026rsquo;t get it, I would rewatch the videos on that particular topic and go back. Here\u0026rsquo;s a repo that i used to keep track of things, and I\u0026rsquo;ve also linked my Notion notes for some of the chapters. Take a look if you like!\nOverall, I feel like \u0026ldquo;Learn Go with Tests\u0026rdquo; was a pretty good experience and helped me pick up a lot of new things, not just language-specific but programming-related in general. It was challenging enough to keep me engaged but not so hard that I wanted to give up. If you\u0026rsquo;re a beginner-intermediate programmer, I think you should definitely give it a try.\ngithub repo used to track progress: https://github.com/sanjayJ369/learningGo\nmy github repo: https://github.com/sanjayJ369\n","permalink":"http://localhost:1313/posts/learn-go-with-tests/","summary":"Part 1: Introduction This is an article about my experience completing \u0026ldquo;Go with Tests.\u0026rdquo; I wanted to learn a back-end programming language for web development, so after completing CS50, I first got started with web development from The Odin Project. However, I only did a few lessons and wasn\u0026rsquo;t very consistent. Additionally, I felt like every three out of five engineering college students were learning the MERN tech stack, so I wanted to do something different.","title":"learn go with tests"}]